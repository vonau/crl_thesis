{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import time\n",
    "import pydde as d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "samplenum = 10\n",
    "epochs = 10\n",
    "hiddenlayers = [90]\n",
    "input_size = 3\n",
    "output_size = 3\n",
    "learning_rate = 0.001\n",
    "time_length = 60; #seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulation\n",
    "dyn = d.PyDyn('Data/point-mass_pendulum.sim', time_length)\n",
    "state_init = dyn.compute(dyn.p_init)\n",
    "f = dyn.f(state_init, dyn.p_init)\n",
    "df = dyn.df_dp(state_init, dyn.p_init)\n",
    "dy = dyn.dy_dp(state_init, dyn.p_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample targets only variables in z direction\n",
    "y_target = np.zeros((samplenum,3))\n",
    "y_target[:,2] = np.random.rand(samplenum)\n",
    "#x[:,0] = np.random.rand(samplenum)\n",
    "y_target[:,1] = 2\n",
    "y_target= torch.tensor(y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the custon Simulation Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LinearFunction(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight):\n",
    "        ctx.save_for_backward(input, weight)\n",
    "        output = input.t().mm(weight).t()\n",
    "        print(f'output: {output}')\n",
    "\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        print(f'grad_output: {grad_output}')\n",
    "        input, weight = ctx.saved_tensors\n",
    "        #grad_input = grad_weight = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        grad_input = grad_output.t().mm(weight.t())\n",
    "        grad_weight = grad_output.mm(input.t()).t()\n",
    "        #print(f'grad_input: {grad_input.shape}')\n",
    "        #print(f'grad_weight: {grad_weight.shape}')\n",
    "        return grad_input, grad_weight\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 10, 3, 100, 60\n",
    "learning_rate = 0.01\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.ones(D_in, N)\n",
    "y = torch.randn(D_out, N)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.ones(D_in, D_out, requires_grad=True)\n",
    "#w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "\n",
    "learning_rate = 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160],\n        [2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160, 2.9160,\n         2.9160]])\ngrad of y_pred: True\n8 174960.078125\ngrad_output: tensor([[100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.]])\noutput: tensor([[2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130],\n        [2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130, 2.9130,\n         2.9130]])\ngrad of y_pred: True\n9 174780.0\ngrad_output: tensor([[100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.],\n        [100., 100., 100., 100., 100., 100., 100., 100., 100., 100.]])\n"
    }
   ],
   "source": [
    "for t in range(10):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    Lin = LinearFunction.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = Lin(x, w1)\n",
    "\n",
    "    # Compute and print loss\n",
    "    #loss = (y_pred - y).pow(2).sum()\n",
    "    loss = y_pred.sum() * 100\n",
    "    print(f'grad of y_pred: {y_pred.requires_grad}')\n",
    "    print(t, loss.item())\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward(retain_graph=True)\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveLearn(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, out_sz):\n",
    "        super(ActiveLearn, self).__init__()\n",
    "\n",
    "        self.L_in = nn.Linear(n_in, hiddenlayers[0])\n",
    "        self.H1 = nn.Linear(hiddenlayers[0], 3*time_length)\n",
    "        #self.H1 = nn.Linear(hiddenlayers[0], hiddenlayers[1])\n",
    "        #self.H2 = nn.Linear(hiddenlayers[1], 3*time_length)\n",
    "        self.P = nn.Linear(3*time_length, 3*time_length)\n",
    "        self.Relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.L_in(input)\n",
    "        x = self.Relu(x)\n",
    "        x = self.H1(x)\n",
    "        x = self.Relu(x)\n",
    "        #x = self.H2(x)\n",
    "        #x = self.Relu(x)\n",
    "        x = self.P(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ActiveLearn(input_size, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()  # RMSE = np.sqrt(MSE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "y_target = y_target.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch:   0/100  loss: 4.29265270\nepoch:   1/100  loss: 1.59549517\nepoch:   2/100  loss: 1.19569693\nepoch:   3/100  loss: 1.06783164\nepoch:   4/100  loss: 0.71800677\nepoch:   5/100  loss: 0.34746730\nepoch:   6/100  loss: 0.24957020\nepoch:   7/100  loss: 0.14124617\nepoch:   8/100  loss: 0.05908810\nepoch:   9/100  loss: 0.06164659\nepoch:  10/100  loss: 0.04696999\nepoch:  11/100  loss: 0.04116096\nepoch:  12/100  loss: 0.03364264\nepoch:  13/100  loss: 0.03126724\nepoch:  14/100  loss: 0.02766821\nepoch:  15/100  loss: 0.02761018\nepoch:  16/100  loss: 0.02515688\nepoch:  17/100  loss: 0.02508319\nepoch:  18/100  loss: 0.02201077\nepoch:  19/100  loss: 0.10844367\nepoch:  20/100  loss: 0.07579721\nepoch:  21/100  loss: 0.06616149\nepoch:  22/100  loss: 0.07300152\nepoch:  23/100  loss: 0.12004107\nepoch:  24/100  loss: 0.10938503\nepoch:  25/100  loss: 0.15107416\nepoch:  26/100  loss: 0.14625780\nepoch:  27/100  loss: 0.13239175\nepoch:  28/100  loss: 0.05033869\nepoch:  29/100  loss: 0.05827785\nepoch:  30/100  loss: 0.10398991\nepoch:  31/100  loss: 0.10143876\nepoch:  32/100  loss: 0.09488417\nepoch:  33/100  loss: 0.08860691\nepoch:  34/100  loss: 0.08308009\nepoch:  35/100  loss: 0.13347684\nepoch:  36/100  loss: 0.13517270\nepoch:  37/100  loss: 0.12787346\nepoch:  38/100  loss: 0.12074984\nepoch:  39/100  loss: 0.11786866\nepoch:  40/100  loss: 0.11247391\nepoch:  41/100  loss: 0.20389264\nepoch:  42/100  loss: 0.19853572\nepoch:  43/100  loss: 0.18754664\nepoch:  44/100  loss: 0.17767402\nepoch:  45/100  loss: 0.16845609\nepoch:  46/100  loss: 0.16032614\nepoch:  47/100  loss: 0.15274998\nepoch:  48/100  loss: 0.14580600\nepoch:  49/100  loss: 0.13933689\nepoch:  50/100  loss: 0.13327548\nepoch:  51/100  loss: 0.12757807\nepoch:  52/100  loss: 0.12219800\nepoch:  53/100  loss: 0.11708067\nepoch:  54/100  loss: 0.11226115\nepoch:  55/100  loss: 0.10773339\nepoch:  56/100  loss: 0.10351822\nepoch:  57/100  loss: 0.09910481\nepoch:  58/100  loss: 0.09537410\nepoch:  59/100  loss: 0.10073794\nepoch:  60/100  loss: 0.14540762\nepoch:  61/100  loss: 0.17395690\nepoch:  62/100  loss: 0.07887812\nepoch:  63/100  loss: 0.17065189\nepoch:  64/100  loss: 0.15581646\nepoch:  65/100  loss: 0.31373327\nepoch:  66/100  loss: 0.28295721\nepoch:  67/100  loss: 0.27035047\nepoch:  68/100  loss: 0.25863938\nepoch:  69/100  loss: 0.24771646\nepoch:  70/100  loss: 0.23751313\nepoch:  71/100  loss: 0.22795422\nepoch:  72/100  loss: 0.21901829\nepoch:  73/100  loss: 0.21059978\nepoch:  74/100  loss: 0.21391559\nepoch:  75/100  loss: 0.19420655\nepoch:  76/100  loss: 0.18765340\nepoch:  77/100  loss: 0.18127888\nepoch:  78/100  loss: 0.17522614\nepoch:  79/100  loss: 0.16944047\nepoch:  80/100  loss: 0.16391134\nepoch:  81/100  loss: 0.15862207\nepoch:  82/100  loss: 0.15355943\nepoch:  83/100  loss: 0.14870492\nepoch:  84/100  loss: 0.14405334\nepoch:  85/100  loss: 0.13958711\nepoch:  86/100  loss: 0.13529762\nepoch:  87/100  loss: 0.13118039\nepoch:  88/100  loss: 0.12721383\nepoch:  89/100  loss: 0.12339097\nepoch:  90/100  loss: 0.11970396\nepoch:  91/100  loss: 0.11614426\nepoch:  92/100  loss: 0.11270419\nepoch:  93/100  loss: 0.10937647\nepoch:  94/100  loss: 0.10615485\nepoch:  95/100  loss: 0.10308297\nepoch:  96/100  loss: 0.10006060\nepoch:  97/100  loss: 0.09714605\nepoch:  98/100  loss: 0.09429485\nepoch:  99/100  loss: 0.09154399\nepoch:  99 loss: 0.09154399\n\nDuration: 46.335 min\n"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "start_time = time.time()\n",
    "weight_c1 = 1 # p start condition\n",
    "weight_c2 = 0.1 # p smoothness condition\n",
    "losses = []\n",
    "y_preds= np.zeros((samplenum, 3))\n",
    "p_preds= np.zeros((samplenum, 3*time_length))\n",
    "\n",
    "#y_pred = torch.tensor(y_pred)\n",
    "for i in range(epochs):\n",
    "    for s in range(samplenum):\n",
    "        y_truth = y_target[s, :]\n",
    "        p_pred = model(y_truth)\n",
    "        y_pred = Simulate(p_pred)\n",
    "        y_preds[s, :] = y_pred.detach()\n",
    "        p_preds[s, :] = p_pred.detach()\n",
    "        #smoothness_error = Smooth(p_pred)\n",
    "        error = 0\n",
    "        smoothness_error = criterion(p_pred[0:3*(time_length-1)], p_pred[3:3*time_length])\n",
    "        loss = torch.sqrt(criterion(y_pred.float(), y_truth)) # RMSE\n",
    "        #loss = criterion(y_pred.float(), y_truth) + weight_c1*(sum(p_pred[0:3]-torch.tensor(dyn.p_init[0:3])))**2  # MSE + start condition penalty\n",
    "        #loss = criterion(y_pred.float(), y_truth) + weight_c1*criterion(p_pred[0:3].double(), torch.tensor(dyn.p_init[0:3])) + weight_c2*smoothness_error# MSE + start condition penalty + p smoothness condition penalty\n",
    "        #loss =sum((y_pred.float()-y_truth)**2) \n",
    "        losses.append(loss)\n",
    "        optimizer.zero_grad()\n",
    "        #Back Prop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch: {i:3}/{epochs}  loss: {loss.item():10.8f}')\n",
    "\n",
    "print(f'epoch: {i:3} loss: {loss.item():10.8f}') # print the last line\n",
    "print(f'\\nDuration: {(time.time() - start_time)/60:.3f} min') # print the time elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.00338275 1.9851979  0.530538  ]\ntensor([0.0000, 2.0000, 0.5000])\np trajecory: [-2.83841975e-03  2.99166989e+00  5.00382856e-04 -4.57676828e-01\n  2.63269997e+00  5.21992743e-01 -6.81326151e-01  2.34795642e+00\n -2.74650395e-01 -5.83646119e-01  1.52864778e+00 -5.56134224e-01\n -4.80328590e-01  1.39862609e+00 -3.67586166e-01  2.40416359e-02\n  4.23111081e-01  5.87768704e-02 -2.33206809e-01  4.84932035e-01\n -1.22672468e-01  2.28466719e-01  2.91416436e-01  2.32087135e-01\n  2.46739164e-01  9.78568010e-03  3.55454922e-01  1.80878788e-01\n  7.32885599e-02  4.53660607e-01  1.05677485e-01  8.67701948e-01\n  5.26671529e-01  1.63849890e-01  4.57568586e-01  3.87044370e-01\n  1.79882601e-01 -3.42988282e-01  2.72964060e-01  8.99747312e-02\n  3.77623886e-01  1.89842135e-01  7.42183533e-03  3.33228946e-01\n -1.25915036e-01 -5.38735166e-02  6.11900687e-01 -4.17560935e-01\n -2.79457569e-01 -4.41163778e-01 -1.97180994e-02 -3.17232817e-01\n -2.63173133e-01 -6.54147416e-02 -2.60562629e-01 -8.31599608e-02\n -1.42009303e-01 -2.45186776e-01 -1.10533036e-01 -2.01573759e-01\n -2.41620690e-01 -1.87139124e-01 -3.49660292e-02  1.44845158e-01\n -2.60630220e-01  1.19555205e-01  5.89642346e-01 -1.78480774e-01\n  2.48251349e-01 -3.09443980e-01  3.30464363e-01  2.90339887e-01\n -4.77115840e-01  2.29622766e-01 -2.82758147e-01 -5.82952738e-01\n  3.96652609e-01 -1.03673208e+00 -1.45638496e-01 -4.70737338e-01\n -1.71435165e+00  1.18847370e+00 -7.83101559e-01  6.09683454e-01\n  2.54720569e-01 -1.09368598e+00  1.24769783e+00 -1.45108855e+00\n  1.08252382e+00  3.27295184e-01 -8.80433738e-01  1.36384141e+00\n -5.27401030e-01 -5.47956347e-01  7.49108195e-01 -1.27250493e+00\n -2.98813492e-01  2.91585356e-01 -1.44998860e+00  1.40009999e+00\n -7.12170124e-01  1.02870390e-01 -1.92454517e+00 -1.66451979e+00\n -5.73510110e-01 -2.15262437e+00  8.42324972e-01  2.98315361e-02\n  4.32337672e-01  1.28401470e+00  1.05874097e+00  4.68583047e-01\n -2.30281770e-01  1.44510555e+00  4.78497297e-02 -1.44499075e+00\n  5.35661638e-01  4.34758961e-01 -1.10476804e+00 -8.81430149e-01\n  3.27372193e-01  2.79954940e-01  2.77479053e-01 -1.06877637e+00\n  4.68140215e-01 -2.28119522e-01  1.33878684e+00  1.23508513e+00\n  4.34727162e-01  1.11215198e+00  1.03021896e+00 -2.35561922e-01\n -2.09827855e-01  4.60227668e-01 -7.77647972e-01  8.81490409e-01\n -4.52910751e-01  6.01446509e-01 -9.87612903e-01 -2.72161037e-01\n  1.35180295e+00 -7.37210631e-01 -1.47744358e+00 -4.42656875e-01\n  1.13714206e+00  1.82901394e+00 -7.23857701e-01 -1.40226698e+00\n  1.03933692e+00  1.96373630e+00 -1.20130658e+00  5.56967020e-01\n  1.21372223e+00 -1.00108528e+00  2.10417077e-01  6.26443326e-01\n -7.34353065e-01  2.05238938e-01 -4.10284474e-02  1.61801279e-01\n  4.62145984e-01 -2.51514256e-01  1.32352793e+00  9.40991879e-01\n -2.53005415e-01 -1.89921439e-01  1.07509172e+00 -2.16544539e-01\n -1.41031551e+00  1.27623904e+00 -1.06323636e+00 -4.59618084e-02\n  2.37056792e-01 -1.12811983e+00  1.29704630e+00  4.81300116e-01\n  1.12023234e+00 -7.26763785e-01  2.82560682e+00  1.97150633e-02]\ny trajectory: [ 3.53124572e-05  1.99220270e+00 -2.39994550e-06  2.10215370e-02\n  1.95216695e+00 -2.39219800e-02  1.34208096e-01  1.85350604e+00\n -1.89311911e-02  2.34794192e-01  1.74873179e+00 -2.21612896e-02\n  4.01557396e-01  1.66091480e+00  2.37030607e-03  4.63678015e-01\n  1.34980130e+00  3.30301545e-02  5.57026747e-01  1.05863411e+00\n  7.13576001e-02  8.57535105e-01  9.97147460e-01  4.95849154e-02\n  9.49846720e-01  7.22032766e-01  1.02665539e-01  1.05481711e+00\n  4.49644490e-01  1.51367525e-01  9.38057457e-01  3.20338360e-01\n  2.68769624e-01  1.06142145e+00  8.99632941e-02  3.85853001e-01\n  1.14160883e+00 -1.51723581e-01  4.93055400e-01  9.13017151e-01\n -1.85041487e-01  4.88289818e-01  6.46485085e-01 -1.90019773e-01\n  4.49370269e-01  3.04959175e-01 -5.77666668e-02  2.67315684e-01\n  1.22523256e-01  4.07720052e-01  1.54020959e-01 -9.32009874e-02\n  7.23826055e-01  2.69952647e-02 -3.04175504e-01  9.27175001e-01\n -1.04153205e-01 -4.63605739e-01  8.91363306e-01 -2.28861979e-01\n -5.77467941e-01  7.28580953e-01 -3.15506756e-01 -5.23939015e-01\n  3.98179096e-01 -2.97712811e-01 -3.04850749e-01  2.70171727e-02\n -1.97429067e-01 -3.92005624e-02 -4.87864769e-01 -1.77804901e-01\n  3.58455793e-02 -6.70870291e-01 -1.91930201e-01 -1.38656212e-01\n -4.05855985e-01 -5.04816442e-01 -3.17970776e-01 -1.34167190e-01\n -7.91942298e-01  4.42742382e-01 -3.77049809e-01 -1.37371691e-01\n  1.01236760e+00 -7.17528893e-01  6.64331190e-01 -4.99763209e-01\n  4.10291773e-01  6.84460399e-01 -1.59539946e+00  1.47224074e+00\n  2.51089473e-01 -1.37962881e+00  1.44079365e+00 -8.49418011e-01\n -9.03507957e-01  1.07103207e+00 -1.79946082e+00  7.99864251e-01\n -2.48822429e-01 -8.33988116e-01 -6.87088021e-01 -1.63849561e+00\n -3.76481260e-01 -2.15935681e+00 -3.75283436e-01  4.59252096e-02\n -7.81874162e-01  1.16485984e+00  8.82341261e-01  5.18498719e-01\n  9.22091348e-01  1.55263922e+00  6.92060014e-01 -6.73261605e-01\n  1.14940915e+00  6.64576618e-01 -1.72700559e+00 -1.32797084e-02\n  4.35494222e-01 -7.89472653e-01 -2.29912084e-01 -8.19935472e-02\n  2.18343079e-01 -3.97143967e-01  3.58487428e-01  1.22919235e+00\n -7.05796457e-02  8.70919102e-01  1.95999838e+00  1.43056911e-01\n  4.13350166e-01  1.33166055e+00 -3.33945903e-01  3.87059997e-01\n  1.63189508e-01 -1.52927727e-01 -2.62187483e-01 -6.68043462e-01\n  6.39650029e-01 -8.37066310e-01 -1.49151877e+00  6.31922533e-01\n  4.88986820e-01  7.74626024e-01 -3.81088944e-01 -5.85600977e-01\n  1.54667975e+00  1.11139888e+00 -1.42410348e+00  1.41101843e+00\n  1.88867615e+00 -1.54734432e+00  6.70373516e-01  1.50958446e+00\n -1.37827947e+00  1.38728393e-02  7.64726298e-01 -6.61687004e-01\n -2.03063150e-01 -8.85108840e-02  6.28731976e-01  1.94115325e-01\n -6.30214255e-01  8.05764203e-01  8.45405964e-01 -6.67572805e-01\n -3.05468822e-01  1.37676133e+00 -8.97814886e-01 -7.45054219e-01\n  1.08792584e+00 -1.12808777e+00  4.54192211e-01  5.88305607e-01\n  2.78428701e-01  3.38274575e-03  1.98519790e+00  5.30537996e-01]\n2.638777479782273\n"
    }
   ],
   "source": [
    "y_target= torch.tensor([0, 2, 0.5])\n",
    "p = model(y_target)\n",
    "p = p.detach().numpy()\n",
    "\n",
    "y_pred_state = dyn.compute(p)\n",
    "y_pred = y_pred_state.y[-3:]\n",
    "\n",
    "print(y_pred)\n",
    "print(y_target)\n",
    "print(f'p trajecory: {p}')\n",
    "print(f\"y trajectory: {y_pred_state.y}\")\n",
    "error = 0\n",
    "for i in range(time_length-1):\n",
    "    step = sum((p[3*i:3*i+3] - p[3*i+3:3*i+6])**2)\n",
    "    error = error + step\n",
    "print(error/time_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Script Conversion and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([-2.8384e-03,  2.9917e+00,  5.0038e-04, -4.5768e-01,  2.6327e+00,\n         5.2199e-01, -6.8133e-01,  2.3480e+00, -2.7465e-01, -5.8365e-01,\n         1.5286e+00, -5.5613e-01, -4.8033e-01,  1.3986e+00, -3.6759e-01,\n         2.4042e-02,  4.2311e-01,  5.8777e-02, -2.3321e-01,  4.8493e-01,\n        -1.2267e-01,  2.2847e-01,  2.9142e-01,  2.3209e-01,  2.4674e-01,\n         9.7857e-03,  3.5545e-01,  1.8088e-01,  7.3289e-02,  4.5366e-01,\n         1.0568e-01,  8.6770e-01,  5.2667e-01,  1.6385e-01,  4.5757e-01,\n         3.8704e-01,  1.7988e-01, -3.4299e-01,  2.7296e-01,  8.9975e-02,\n         3.7762e-01,  1.8984e-01,  7.4218e-03,  3.3323e-01, -1.2592e-01,\n        -5.3874e-02,  6.1190e-01, -4.1756e-01, -2.7946e-01, -4.4116e-01,\n        -1.9718e-02, -3.1723e-01, -2.6317e-01, -6.5415e-02, -2.6056e-01,\n        -8.3160e-02, -1.4201e-01, -2.4519e-01, -1.1053e-01, -2.0157e-01,\n        -2.4162e-01, -1.8714e-01, -3.4966e-02,  1.4485e-01, -2.6063e-01,\n         1.1956e-01,  5.8964e-01, -1.7848e-01,  2.4825e-01, -3.0944e-01,\n         3.3046e-01,  2.9034e-01, -4.7712e-01,  2.2962e-01, -2.8276e-01,\n        -5.8295e-01,  3.9665e-01, -1.0367e+00, -1.4564e-01, -4.7074e-01,\n        -1.7144e+00,  1.1885e+00, -7.8310e-01,  6.0968e-01,  2.5472e-01,\n        -1.0937e+00,  1.2477e+00, -1.4511e+00,  1.0825e+00,  3.2730e-01,\n        -8.8043e-01,  1.3638e+00, -5.2740e-01, -5.4796e-01,  7.4911e-01,\n        -1.2725e+00, -2.9881e-01,  2.9159e-01, -1.4500e+00,  1.4001e+00,\n        -7.1217e-01,  1.0287e-01, -1.9245e+00, -1.6645e+00, -5.7351e-01,\n        -2.1526e+00,  8.4232e-01,  2.9832e-02,  4.3234e-01,  1.2840e+00,\n         1.0587e+00,  4.6858e-01, -2.3028e-01,  1.4451e+00,  4.7850e-02,\n        -1.4450e+00,  5.3566e-01,  4.3476e-01, -1.1048e+00, -8.8143e-01,\n         3.2737e-01,  2.7995e-01,  2.7748e-01, -1.0688e+00,  4.6814e-01,\n        -2.2812e-01,  1.3388e+00,  1.2351e+00,  4.3473e-01,  1.1122e+00,\n         1.0302e+00, -2.3556e-01, -2.0983e-01,  4.6023e-01, -7.7765e-01,\n         8.8149e-01, -4.5291e-01,  6.0145e-01, -9.8761e-01, -2.7216e-01,\n         1.3518e+00, -7.3721e-01, -1.4774e+00, -4.4266e-01,  1.1371e+00,\n         1.8290e+00, -7.2386e-01, -1.4023e+00,  1.0393e+00,  1.9637e+00,\n        -1.2013e+00,  5.5697e-01,  1.2137e+00, -1.0011e+00,  2.1042e-01,\n         6.2644e-01, -7.3435e-01,  2.0524e-01, -4.1028e-02,  1.6180e-01,\n         4.6215e-01, -2.5151e-01,  1.3235e+00,  9.4099e-01, -2.5301e-01,\n        -1.8992e-01,  1.0751e+00, -2.1654e-01, -1.4103e+00,  1.2762e+00,\n        -1.0632e+00, -4.5962e-02,  2.3706e-01, -1.1281e+00,  1.2970e+00,\n         4.8130e-01,  1.1202e+00, -7.2676e-01,  2.8256e+00,  1.9715e-02],\n       grad_fn=<AddBackward0>)\n"
    }
   ],
   "source": [
    "input_example = torch.tensor(y_target)\n",
    "traced_script_module = torch.jit.trace(model, input_example)\n",
    "test= traced_script_module(y_target)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save serialized model\n",
    "traced_script_module.save(\"CPP_example_model_latest.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the torch script\n",
    "test_input= torch.tensor([0, 2, 0.5])\n",
    "output_example = traced_script_module(test_input)\n",
    "print(output_example[-12:])\n",
    "print(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model saved\n"
    }
   ],
   "source": [
    "#Save Model\n",
    "\n",
    "if len(losses) == epochs*(samplenum):\n",
    "    torch.save(model.state_dict(), 'Trained_Model_300420_300s_100e_onlyZpos.pt')\n",
    "    print('Model saved')\n",
    "else:\n",
    "    print('Model has not been trained. Consider loading a trained model instead.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bitc9de6dffaec048baa4256f94fcb6712f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}