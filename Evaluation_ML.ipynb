{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import pydde as d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Processing\n",
    "target = np.array([0, 2, 0.9])\n",
    "target = torch.tensor(target).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (bnorm): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=200, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=100, out_features=180, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and Sim\n",
    "\n",
    "# Generate simulation\n",
    "sim = d.PySimSeq('test2.sim', 60)\n",
    "yseq = sim.compute(sim.p)\n",
    "f = sim.f(sim.y, sim.ydot, sim.yddot, sim.p)\n",
    "df = sim.df_dp(sim.y, sim.ydot, sim.yddot, sim.p)\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, out_sz, hlayers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bnorm = nn.BatchNorm1d(n_in)\n",
    "\n",
    "        layerlist = []\n",
    "\n",
    "        for i in hlayers:\n",
    "            layerlist.append(nn.Linear(n_in,i))\n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            #layerlist.append(nn.BatchNorm1d(i))\n",
    "            n_in = i\n",
    "        layerlist.append(nn.Linear(hlayers[-1],out_sz))\n",
    "\n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #x = self.bnorm(x)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "#Load model\n",
    "model_eval = Model(3, 180, [200,100])\n",
    "model_eval.load_state_dict(torch.load('Trained_Model5000z.pt'));\n",
    "model_eval.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    p = model_eval(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store p traj\n",
    "sim.save_ptraj('p_traj.p', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_0: [-8.34951326e-05  2.02299677e+00  1.69111837e-03]\n",
      "\n",
      "y_end: [-0.00816879  2.02603096  0.89845623]\n",
      "\n",
      "Diff: [0.00816879 0.02603096 0.00154374]\n",
      "\n",
      "Error: 0.0012775972791816523\n"
     ]
    }
   ],
   "source": [
    "ysim = sim.compute(p)\n",
    "\n",
    "print(f'\\ny_0: {ysim[0:3]}') \n",
    "print(f'\\ny_end: {ysim[59*3:]}') \n",
    "print(f'\\nDiff: {(abs(ysim[59*3:]-np.array(target)))}') \n",
    "print(f'\\nError: {sum(abs(ysim[59*3:]-np.array(target)))**2}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
