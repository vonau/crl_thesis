{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "nTimeSteps: 60\nusing dde version: v0.1.2-20-g1cc6119-dirty\nlog level set to LogLevel.off\n['data_1.json', 'qhold.out', '.DS_Store', 'data_0.json', 'data_10.json', 'data_7.json', 'data_test', 'data_6.json', 'qhold_active2.out', 'data_11.json', 'Description.json', 'data_9.json', 'data_12.json', 'data_5.json', 'data_4.json', 'data_8.json', 'data_13.json', 'data_3.json', 'data_14.json', 'data_2.json']\n"
    }
   ],
   "source": [
    "########################################\n",
    "#LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import time\n",
    "import pydde as dde\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "########################################\n",
    "#PARAMETERS\n",
    "nTimeSteps = 60; #at 60 Hz\n",
    "print(f'nTimeSteps: {nTimeSteps}')\n",
    "input_size = 9\n",
    "use_case = 'qhold'\n",
    "learning_rate = 0.001\n",
    "LRdecay = 0.7\n",
    "model_file_path = '../Trained_Models/'\n",
    "sample_file_path = f'/Users/nicovonau/Code/thesis_pytorch/Data/Samples/data_{use_case}_{nTimeSteps}tsteps_1024/'\n",
    "simulation_file_path = '/Users/nicovonau/Code/thesis_pytorch/Data/Simulations/pm_target.sim'\n",
    "objective_file_path = f'/Users/nicovonau/Code/thesis_pytorch/Data/Objectives/pm_{use_case}.obj'\n",
    "# check dde version\n",
    "print(\"using dde version: \" + dde.__version__)\n",
    "# set log level\n",
    "dde.set_log_level(dde.LogLevel.off)\n",
    "print(f'log level set to {dde.get_log_level()}')\n",
    "\n",
    "print(os.listdir(sample_file_path))\n",
    "\n",
    "dyn = dde.DynamicSequence()\n",
    "dyn.loadFile(simulation_file_path, nTimeSteps)\n",
    "output_size = dyn.nParameters*nTimeSteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Shape of input: (15000, 9)\nShape of p: (15000, 180)\n(15000, 9)\n(15000, 180)\nShape of input_test: (5000, 9)\nShape of p_test: (5000, 180)\n(5000, 9)\n(5000, 180)\n"
    }
   ],
   "source": [
    "#########################################\n",
    "#LOAD TRAINING SAMPLES\n",
    "number_of_files = len(os.listdir(sample_file_path))-5\n",
    "samplenum = 1000*number_of_files\n",
    "output_size = dyn.nParameters*nTimeSteps\n",
    "\n",
    "p = np.zeros((samplenum, dyn.nParameters*nTimeSteps))\n",
    "input = np.zeros((samplenum, input_size))\n",
    "\n",
    "for filenum in range(number_of_files):\n",
    "    with open(sample_file_path + f'data_{filenum}.json') as json_file:\n",
    "        data_ = json.load(json_file)\n",
    "        filesize = len(data_['q'])\n",
    "        for i, p_i in enumerate(data_['p']):\n",
    "            p[filenum*filesize+i, :] = np.array(p_i)\n",
    "        for i, q_i in enumerate(data_['q']):\n",
    "            input[filenum*filesize+i, 0:3] = np.array(q_i)\n",
    "        for i, qdot_i in enumerate(data_['qdot']):\n",
    "            input[filenum*filesize+i, 3:6] = np.array(qdot_i)\n",
    "        for i, p_now_i in enumerate(data_['p_now']):\n",
    "            input[filenum*filesize+i, 6:9] = np.array(p_now_i)\n",
    "\n",
    "print(f'Shape of input: {input.shape}')\n",
    "print(f'Shape of p: {p.shape}')\n",
    "#Remove zeros\n",
    "data = input[~(input == 0).all(1)]\n",
    "p = p[~(p == 0).all(1)]\n",
    "print(data.shape)\n",
    "print(p.shape)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#y_train, y_test, p_train, p_test = train_test_split(y_target, p, test_size = testsize)\n",
    "\n",
    "#y_target = torch.tensor(y_train).float()\n",
    "#p = torch.tensor(p_train).float()\n",
    "p = torch.tensor(p).float()\n",
    "#y_test = torch.tensor(y_test).float()\n",
    "#p_test = torch.tensor(p_test).float()\n",
    "\n",
    "#########################################\n",
    "#LOAD TEST SAMPLES\n",
    "number_of_files_test = len(os.listdir(sample_file_path + 'data_test/'))\n",
    "samplenum_test = 1000*number_of_files_test\n",
    "\n",
    "p_test = np.zeros((samplenum_test, 3*nTimeSteps))\n",
    "input_test = np.zeros((samplenum_test, input_size))\n",
    "\n",
    "for filenum in range(number_of_files_test):\n",
    "    with open(sample_file_path + f'data_test/data_{filenum}.json') as json_file:\n",
    "        data_ = json.load(json_file)\n",
    "        filesize = len(data_['q'])\n",
    "        for i, p_i in enumerate(data_['p']):\n",
    "            p_test[filenum*filesize+i, :] = np.array(p_i)\n",
    "        for i, q_i in enumerate(data_['q']):\n",
    "            input_test[filenum*filesize+i, 0:3] = np.array(q_i)\n",
    "        for i, qdot_i in enumerate(data_['qdot']):\n",
    "            input_test[filenum*filesize+i, 3:6] = np.array(qdot_i)\n",
    "        for i, p_now_i in enumerate(data_['p_now']):\n",
    "            input_test[filenum*filesize+i, 6:9] = np.array(p_now_i)\n",
    "\n",
    "print(f'Shape of input_test: {input_test.shape}')\n",
    "print(f'Shape of p_test: {p_test.shape}')\n",
    "#Remove zeros\n",
    "input_test = input_test[~(input_test == 0).all(1)]\n",
    "p_test = p_test[~(p_test == 0).all(1)]\n",
    "print(input_test.shape)\n",
    "print(p_test.shape)\n",
    "\n",
    "input_test = torch.tensor(input_test).float()\n",
    "p_test = torch.tensor(p_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "learning_rate = 0.001\n",
    "epochs_s = [300]\n",
    "minibatch_size = [30]\n",
    "LRdecay_s = [0.5]\n",
    "hiddenlayers_s = [[150, 200], [200, 250]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch:   0/10    LR:   0.001000  loss: 0.01218206\nepoch:   1/10    LR:   0.001000  loss: 0.00740917\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-33a9155e09c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                         \u001b[0;31m#Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "#BUILD CUSTOM SIMULATION FUNCTION\n",
    "class Simulate(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, data_input):\n",
    "        #print(f'input: {input.shape}')\n",
    "        p = input.detach().clone().numpy()\n",
    "        q_pred = torch.ones([len(p[:, 0]),dyn.nParameters*nTimeSteps])\n",
    "        for i in range(len(p[:, 0])):\n",
    "            dyn.q0 = data_input[i, 0:3]\n",
    "            dyn.qdot0 = data_input[i, 3:6]\n",
    "            dyn.po = data_input[i, 6:9]\n",
    "            state = dyn.q(p[i, :])\n",
    "            q_pred[i, :] = torch.tensor(state.q)\n",
    "        #print(f'q_pred: {q_pred.shape}')\n",
    "        data_input_ = torch.tensor(data_input)\n",
    "        ctx.save_for_backward(input, data_input_)\n",
    "        \n",
    "        return q_pred\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        #print(grad_output)\n",
    "        input, data_input = ctx.saved_tensors\n",
    "        p = input.detach().clone().numpy()\n",
    "        data_input = data_input.detach().clone().numpy()\n",
    "        dq_dp_batch = torch.zeros([dyn.nDofs*nTimeSteps, dyn.nParameters*nTimeSteps])\n",
    "        for i in range(len(p[:, 0])):\n",
    "            dyn.q0 = data_input[i, 0:3]\n",
    "            dyn.qdot0 = data_input[i, 3:6]\n",
    "            dyn.po = data_input[i, 6:9]\n",
    "            state = dyn.q(p[i, :])\n",
    "            dq_dp = dyn.dq_dp(state, p[i, :])\n",
    "            dq_dp = torch.tensor(dq_dp)\n",
    "            dq_dp_batch = dq_dp_batch + dq_dp\n",
    "        #print(f'dq/dp_batch: {dy_dp_batch/samplenum}')\n",
    "        grad_input = grad_output.mm(dq_dp_batch.float()/len(p[:,0]))\n",
    "        #print(f'shape of grad input: {grad_input.shape}')\n",
    "        #print(f'shape of grad output: {grad_output.shape}')\n",
    "        return grad_input, None\n",
    "\n",
    "Simulate = Simulate.apply\n",
    "\n",
    "start_time = time.time()\n",
    "weight_c1 = 1 # q error\n",
    "weight_c2 = 1 # p start condition\n",
    "weight_c3 = 1 # p smoothness condition\n",
    "weight_c4 = 100 # p smoothness condition\n",
    "smoothness_errors_p = []\n",
    "smoothness_errors_q = []\n",
    "p_start_errors = []\n",
    "tot_losses = []\n",
    "for e1 in epochs_s:\n",
    "    for b1 in minibatch_size:\n",
    "        for d in LRdecay_s:\n",
    "            for h in hiddenlayers_s:\n",
    "                class ActiveLearn(nn.Module):\n",
    "\n",
    "                    def __init__(self, n_in, out_sz):\n",
    "                        super(ActiveLearn, self).__init__()\n",
    "\n",
    "                        self.L_in = nn.Linear(n_in, h[0])\n",
    "                        self.H1 = nn.Linear(h[0], h[1])\n",
    "                        self.H2 = nn.Linear(h[1], out_sz)\n",
    "                        #self.H3 = nn.Linear(h[2], 3*time_length)\n",
    "                        self.L_out = nn.Linear(out_sz, out_sz)\n",
    "                        self.Relu = nn.ReLU(inplace=True)\n",
    "                        #self.drop = nn.Dropout(p=0.3)\n",
    "                        #self.norm1 = nn.BatchNorm2d(h[0])\n",
    "                        #self.norm2 = nn.BatchNorm2d(h[1])\n",
    "                    \n",
    "                    def forward(self, input):\n",
    "                        x = self.L_in(input)\n",
    "                        #x = self.norm1(x)\n",
    "                        #x = self.drop(x)\n",
    "                        x = self.Relu(x)\n",
    "                        x = self.H1(x)\n",
    "                        #x = self.norm2(x)\n",
    "                        x = self.Relu(x)\n",
    "                        x = self.H2(x)\n",
    "                        x = self.Relu(x)\n",
    "                        #x = self.H3(x)\n",
    "                        #x = self.Relu(x)\n",
    "                        x = self.L_out(x)\n",
    "                        return x\n",
    "\n",
    "\n",
    "                model = ActiveLearn(input_size, output_size)\n",
    "\n",
    "                criterion = nn.SmoothL1Loss()  # RMSE = np.sqrt(MSE)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "                scheduler= torch.optim.lr_scheduler.StepLR(optimizer, step_size = 15, gamma=d, last_epoch=-1)\n",
    "                #train\n",
    "                torch.autograd.set_detect_anomaly(True)\n",
    "                #weight_c1 = 1 # q error\n",
    "                weight_c2 = 1 # p start condition\n",
    "                weight_c3 = 1 # p smoothness condition\n",
    "                weight_c4 = 100 # q smoothness condition\n",
    "                batch = np.floor(samplenum/b1).astype(int)\n",
    "                losses= []\n",
    "                p_smoothness_errors = []\n",
    "                basic_errors = [] #y_end_ and p_start error\n",
    "                for e in range(e1):\n",
    "                    for b in range(batch):\n",
    "                        input_numpy = data[b*b1:b*b1+b1,:]\n",
    "                        input_tensor = torch.tensor(data[b*b1:b*b1+b1,:], requires_grad = True).float()\n",
    "                        p_b = model(input_tensor)\n",
    "                        q_pred = Simulate(p_b, input_numpy)\n",
    "                        # separate losses\n",
    "                        smoothness_error_p = weight_c3*criterion(p_b[:, 0:dyn.nParameters*(nTimeSteps-1)], p_b[:, dyn.nParameters:dyn.nParameters*nTimeSteps])\n",
    "                        smoothness_error_q = weight_c4*criterion(q_pred[:, 0:dyn.nDofs*(nTimeSteps-1)], q_pred[:, dyn.nDofs:dyn.nDofs*nTimeSteps])\n",
    "                        p_start_error = weight_c2*criterion(p_b[:, 0:dyn.nParameters], input_tensor[:,6:9])\n",
    "                        # sum up losses\n",
    "                        loss = p_start_error + smoothness_error_p + smoothness_error_q\n",
    "                        losses.append(loss)\n",
    "                        smoothness_errors_p.append(smoothness_error_p)\n",
    "                        smoothness_errors_q.append(smoothness_error_q)\n",
    "                        p_start_errors.append(p_start_error)\n",
    "                        optimizer.zero_grad()\n",
    "                        #Backpropagation\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    LR= scheduler.get_last_lr()\n",
    "                    if e%(e1/10) == 0:\n",
    "                        print(f'epoch: {e:3}/{e1}    LR: {LR[0]:10.6f}  loss: {loss.item():10.8f}')\n",
    "                    \n",
    "                print(f'Model {index} trained')\n",
    "                print(f'epoch: {e:3} final loss: {loss.item():10.8f}') # print the last line\n",
    "                print(f'Training completed. Total duration: {(time.time() - start_time)/60:.3f} min') # print the time elapsed\n",
    "\n",
    "\n",
    "                #Test the data\n",
    "                #model.eval()\n",
    "                losses_test= []\n",
    "                with torch.no_grad():\n",
    "                    for i in range(samplenum_target_test):\n",
    "                        p_val = model(input_test[i, :])\n",
    "                        loss2 = criterion(p_val,p_test[i,:])\n",
    "                        losses_test.append(loss2.clone().numpy())\n",
    "                #plot test errors\n",
    "                plot = plt.plot(losses, label = 'loss', linewidth=3)\n",
    "                plt.legend()\n",
    "                plt.yscale('log')\n",
    "                plt.ylabel('error')\n",
    "                plt.xlabel('batches')\n",
    "                plt.savefig(f'../GridSearch_scores/{index}_Loss_.png')\n",
    "                tot_error = sum(losses_test)\n",
    "                mean_error = np.mean(losses_test)\n",
    "                scores.append([index, tot_error, np.double(mean_error), e1, b1, d, h])\n",
    "                index = index + 1\n",
    "                print(f'TOTAL ERROR: {tot_error}    mean error: {mean_error}  epochs: {e1}    batchsize: {b1}   LRdecay: {d}  hiddenlayer{h}')\n",
    "                print(\"\\nNEXT MODEL\")\n",
    "print(f'\\nDuration: {(time.time() - start_time)/60:.3f} min') # print the time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Gridsearch_scores_1layer_2.json', 'w') as outfile:\n",
    "    json.dump(tot_losses, outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda39d998b0540f4e659e09acca95c25f2f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}