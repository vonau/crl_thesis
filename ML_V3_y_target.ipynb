{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import pydde as d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "samplenum = 100\n",
    "epochs = 50\n",
    "hiddenlayers = [200,100]\n",
    "input_size = 3\n",
    "output_size = 3\n",
    "learning_rate = 0.01\n",
    "time_length = 60; #seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulation\n",
    "dyn = d.PyDyn('test2.sim', time_length)\n",
    "state_init = dyn.compute(dyn.p_init)\n",
    "f = dyn.py_f(state_init)\n",
    "df = dyn.py_df_dp(state_init)\n",
    "dy = dyn.py_dy_dp(state_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample targets only variables in z direction\n",
    "y_target = np.zeros((samplenum,3))\n",
    "y_target[:,2] = np.random.rand(samplenum)\n",
    "#x[:,0] = np.random.rand(samplenum)\n",
    "y_target[:,1] = 2\n",
    "y_target= torch.tensor(y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the custon Simulation Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulate(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        #print(f'input: {input.shape}')\n",
    "        p = input.clone().numpy().transpose()\n",
    "        state = dyn.compute(p)\n",
    "        y_pred = torch.tensor(state.y[-3:])\n",
    "        #print(f'y_pred: {y_pred.dtype}')\n",
    "        \n",
    "        ctx.save_for_backward(input)\n",
    "        \n",
    "        return y_pred, input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, input):\n",
    "        #print(grad_output.shape)\n",
    "        input, = ctx.saved_tensors\n",
    "        p = input.clone().numpy().transpose()\n",
    "        state= dyn.compute(p)\n",
    "        dy_dp = dyn.py_dy_dp(state)\n",
    "        dy_dp = dy_dp[-3:, :]\n",
    "        #print(f'shape of dy/dp: {dy_dp.shape}')\n",
    "        #print(f'shape of grad_output: {grad_output.shape}')\n",
    "        grad_output = grad_output.unsqueeze(0)\n",
    "        \n",
    "        grad_input = torch.tensor(dy_dp).t().mm(grad_output.t()).t()\n",
    "        return grad_input, None\n",
    "\n",
    "Simulate = Simulate.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveLearn(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, out_sz):\n",
    "        super(ActiveLearn, self).__init__()\n",
    "\n",
    "        self.L_in = nn.Linear(n_in, hiddenlayers[0])\n",
    "        self.H1 = nn.Linear(hiddenlayers[0], hiddenlayers[1])\n",
    "        self.H2 = nn.Linear(hiddenlayers[1], 3*time_length)\n",
    "        self.P = nn.Linear(3*time_length, 3*time_length)\n",
    "        self.Relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.L_in(input)\n",
    "        x = self.Relu(x)\n",
    "        x = self.H1(x)\n",
    "        x = self.Relu(x)\n",
    "        x = self.H2(x)\n",
    "        x = self.Relu(x)\n",
    "        x = self.P(x)\n",
    "        x = self.Relu(x)\n",
    "        x, p = Simulate(x)\n",
    "        return x, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActiveLearn(input_size, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()  # RMSE = np.sqrt(MSE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "y_target = y_target.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch:   0/50  loss: 0.07157972\nepoch:   1/50  loss: 0.46903411\nepoch:   2/50  loss: 0.45481959\nepoch:   3/50  loss: 0.58691806\nepoch:   4/50  loss: 0.45433557\nepoch:   5/50  loss: 0.45458665\nepoch:   6/50  loss: 0.45459202\nepoch:   7/50  loss: 0.45458087\nepoch:   8/50  loss: 0.45457342\nepoch:   9/50  loss: 0.45451936\nepoch:  10/50  loss: 0.45442867\nepoch:  11/50  loss: 0.45418772\nepoch:  12/50  loss: 0.45336437\nepoch:  13/50  loss: 0.45250466\nepoch:  14/50  loss: 0.45262125\nepoch:  15/50  loss: 0.45224699\nepoch:  16/50  loss: 0.45437038\nepoch:  17/50  loss: 0.45241880\nepoch:  18/50  loss: 0.45345151\nepoch:  19/50  loss: 0.45307079\nepoch:  20/50  loss: 0.32630631\nepoch:  21/50  loss: 0.31644014\nepoch:  22/50  loss: 0.31524813\nepoch:  23/50  loss: 0.31430665\nepoch:  24/50  loss: 0.31320596\nepoch:  25/50  loss: 0.31284019\nepoch:  26/50  loss: 0.31278601\nepoch:  27/50  loss: 0.31277874\nepoch:  28/50  loss: 0.31277484\nepoch:  29/50  loss: 0.31277397\nepoch:  30/50  loss: 0.31277373\nepoch:  31/50  loss: 0.31277600\nepoch:  32/50  loss: 0.31277329\nepoch:  33/50  loss: 0.31277308\nepoch:  34/50  loss: 0.31277561\nepoch:  35/50  loss: 0.31279334\nepoch:  36/50  loss: 0.31282979\nepoch:  37/50  loss: 0.31278399\nepoch:  38/50  loss: 0.21173464\nepoch:  39/50  loss: 0.21807615\nepoch:  40/50  loss: 0.13449220\nepoch:  41/50  loss: 0.12477377\nepoch:  42/50  loss: 0.16848415\nepoch:  43/50  loss: 0.17468382\nepoch:  44/50  loss: 0.11245087\nepoch:  45/50  loss: 0.12128413\nepoch:  46/50  loss: 0.20255224\nepoch:  47/50  loss: 0.11222156\nepoch:  48/50  loss: 0.21641301\nepoch:  49/50  loss: 0.17763190\nepoch:  50 loss: 0.17763190\n\nDuration: 277.932 min\n"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "losses = []\n",
    "y_preds= np.zeros((samplenum, 3))\n",
    "p_preds= np.zeros((samplenum, 3*time_length))\n",
    "\n",
    "#y_pred = torch.tensor(y_pred)\n",
    "for i in range(epochs):\n",
    "    for s in range(samplenum):\n",
    "        y_truth = y_target[s, :]\n",
    "        #print(y_truth.shape)\n",
    "        #y_truth = y_truth.unsqueeze(0)\n",
    "        y_pred, p_pred = model(y_truth)\n",
    "        #print(y_pred.shape)\n",
    "        y_preds[s, :] = y_pred.detach()\n",
    "        p_preds[s, :] = p_pred.detach()\n",
    "        #loss = torch.sqrt(criterion(y_pred.float(), y_truth)) # RMSE\n",
    "        loss = criterion(y_pred.float(), y_truth) # MSE\n",
    "        #loss =sum((y_pred.float()-y_truth)**2) \n",
    "        losses.append(loss)\n",
    "        #Clear the gradient buffer (w <-- w - lr*gradient)\n",
    "        optimizer.zero_grad()\n",
    "        #Back Prop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch: {i:3}/{epochs}  loss: {loss.item():10.8f}')\n",
    "    i+=1\n",
    "\n",
    "print(f'epoch: {i:3} loss: {loss.item():10.8f}') # print the last line\n",
    "print(f'\\nDuration: {(time.time() - start_time)/60:.3f} min') # print the time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Model\n",
    "\n",
    "if len(losses) == epochs*(samplenum):\n",
    "    torch.save(model.state_dict(), 'Trained_Model_dyn_100s100e.pt')\n",
    "    print('Model saved')\n",
    "else:\n",
    "    print('Model has not been trained. Consider loading a trained model instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradcheck takes a tuple of tensors as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "input = (x_train, y_train)\n",
    "test = gradcheck(Simulate, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p= np.random.rand(180)\n",
    "p= torch.tensor(p)\n",
    "y_pred, p_pred = Simulate(p)\n",
    "y_pred = y_pred.clone().numpy()\n",
    "\n",
    "yTraj_test = sim.compute(p_pred)\n",
    "\n",
    "print(y_pred)\n",
    "print(yTraj_test[-3:])\n",
    "print(np.sum(yTraj_test[-3:]-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}