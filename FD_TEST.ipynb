{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import pydde as d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "samplenum = 50\n",
    "epochs = 200\n",
    "input_size = 3\n",
    "output_size = 3\n",
    "learning_rate = 0.01\n",
    "time_length = 60; #seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulation\n",
    "sim = d.PySimSeq('test2.sim', 60)\n",
    "yseq = sim.compute(sim.p)\n",
    "f = sim.f(sim.y, sim.ydot, sim.yddot, sim.p)\n",
    "df = sim.df_dp(sim.y, sim.ydot, sim.yddot, sim.p)\n",
    "dy = sim.dy_dp(sim.y, sim.ydot, sim.yddot, sim.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the custon Simulation Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulate(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        #print(f'input: {input.shape}')\n",
    "        p = input.clone().numpy().transpose()\n",
    "        yTraj_pred = sim.compute(p)\n",
    "        y_pred = torch.tensor(yTraj_pred[-3:]).double()\n",
    "        #print(f'y_pred: {y_pred.dtype}')\n",
    "        \n",
    "        ctx.save_for_backward(input)\n",
    "        \n",
    "        return y_pred, input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, input):\n",
    "        #print(grad_output.shape)\n",
    "        input, = ctx.saved_tensors\n",
    "        p = input.clone().numpy().transpose()\n",
    "        yTraj_pred = sim.compute(p)\n",
    "        dy_dp = sim.dy_dp(sim.y, sim.ydot, sim.yddot, p)\n",
    "        dy_dp = dy_dp[-3:, :]\n",
    "        grad_output = grad_output.unsqueeze(0)\n",
    "        \n",
    "        grad_input = torch.tensor(dy_dp).t().mm(grad_output.t()).t().double()\n",
    "        return grad_input, None\n",
    "\n",
    "Simulate = Simulate.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([ 6.1626e-04,  6.2852e-08,  6.1627e-04,  3.1267e-03,  2.7846e-07,\n         3.1267e-03,  5.6354e-03,  4.9105e-07,  5.6355e-03,  8.1357e-03,\n         6.9829e-07,  8.1358e-03,  1.0621e-02,  8.9795e-07,  1.0621e-02,\n         1.3083e-02,  1.0879e-06,  1.3083e-02,  1.5517e-02,  1.2659e-06,\n         1.5517e-02,  1.7915e-02,  1.4303e-06,  1.7916e-02,  2.0271e-02,\n         1.5790e-06,  2.0271e-02,  2.2578e-02,  1.7107e-06,  2.2579e-02,\n         2.4830e-02,  1.8237e-06,  2.4830e-02,  2.7020e-02,  1.9169e-06,\n         2.7021e-02,  2.9143e-02,  1.9893e-06,  2.9144e-02,  3.1192e-02,\n         2.0400e-06,  3.1193e-02,  3.3162e-02,  2.0686e-06,  3.3162e-02,\n         3.5046e-02,  2.0748e-06,  3.5047e-02,  3.6841e-02,  2.0583e-06,\n         3.6842e-02,  3.8540e-02,  2.0195e-06,  3.8541e-02,  4.0138e-02,\n         1.9588e-06,  4.0139e-02,  4.1632e-02,  1.8767e-06,  4.1633e-02,\n         4.3016e-02,  1.7743e-06,  4.3017e-02,  4.4287e-02,  1.6526e-06,\n         4.4288e-02,  4.5440e-02,  1.5129e-06,  4.5441e-02,  4.6473e-02,\n         1.3568e-06,  4.6474e-02,  4.7383e-02,  1.1860e-06,  4.7384e-02,\n         4.8165e-02,  1.0022e-06,  4.8166e-02,  4.8819e-02,  8.0759e-07,\n         4.8820e-02,  4.9341e-02,  6.0418e-07,  4.9342e-02,  4.9730e-02,\n         3.9420e-07,  4.9731e-02,  4.9985e-02,  1.7994e-07,  4.9986e-02,\n         5.0105e-02, -3.6271e-08,  5.0106e-02,  5.0088e-02, -2.5209e-07,\n         5.0089e-02,  4.9935e-02, -4.6517e-07,  4.9936e-02,  4.9645e-02,\n        -6.7320e-07,  4.9646e-02,  4.9219e-02, -8.7391e-07,  4.9220e-02,\n         4.8658e-02, -1.0651e-06,  4.8659e-02,  4.7963e-02, -1.2448e-06,\n         4.7964e-02,  4.7136e-02, -1.4109e-06,  4.7137e-02,  4.6177e-02,\n        -1.5616e-06,  4.6178e-02,  4.5091e-02, -1.6954e-06,  4.5092e-02,\n         4.3878e-02, -1.8109e-06,  4.3879e-02,  4.2543e-02, -1.9075e-06,\n         4.2544e-02,  4.1088e-02, -1.9829e-06,  4.1089e-02,  3.9517e-02,\n        -2.0288e-06,  3.9518e-02,  3.7835e-02, -2.0398e-06,  3.7835e-02,\n         3.6044e-02, -2.0847e-06,  3.6045e-02,  3.4151e-02, -2.3467e-06,\n         3.4152e-02,  3.2159e-02, -2.5003e-06,  3.2160e-02,  3.0075e-02,\n        -5.3062e-08,  3.0075e-02,  2.7903e-02,  6.7082e-06,  2.7903e-02,\n         2.5648e-02, -3.8186e-06,  2.5649e-02,  2.3318e-02, -9.2049e-05,\n         2.3319e-02,  2.0918e-02, -1.6190e-04,  2.0919e-02,  1.8455e-02,\n         5.8545e-04,  1.8455e-02,  1.5934e-02,  2.7828e-03,  1.5935e-02,\n         1.3363e-02, -3.2564e-04,  1.3364e-02,  1.0749e-02, -2.8606e-02,\n         1.0749e-02,  8.0975e-03, -5.3951e-02,  8.0987e-03,  5.4222e-03,\n         1.7934e-01,  5.4188e-03,  2.7345e-03,  9.0043e-01,  2.7168e-03],\n       dtype=torch.float64)\n[[ 6.16260259e-04  2.68182691e-07  6.16272897e-04  3.12668239e-03\n  -4.95156315e-07  3.12674453e-03  5.63542579e-03 -1.25311449e-06\n   5.63553732e-03  8.13565138e-03 -1.99745472e-06  8.13581230e-03\n   1.06205251e-02 -2.71997754e-06  1.06207351e-02  1.30832361e-02\n  -3.41305190e-06  1.30834944e-02  1.55170146e-02 -4.06903494e-06\n   1.55173210e-02  1.79151522e-02 -4.68079822e-06  1.79155058e-02\n   2.02710186e-02 -5.24169358e-06  2.02714187e-02  2.25780810e-02\n  -5.74562547e-06  2.25785265e-02  2.48299213e-02 -6.18711792e-06\n   2.48304114e-02  2.70202552e-02 -6.56137312e-06  2.70207883e-02\n   2.91429477e-02 -6.86432393e-06  2.91435227e-02  3.11920322e-02\n  -7.09267813e-06  3.11926477e-02  3.31617263e-02 -7.24395422e-06\n   3.31623805e-02  3.50464476e-02 -7.31650826e-06  3.50471390e-02\n   3.68408303e-02 -7.30955184e-06  3.68415573e-02  3.85397405e-02\n  -7.22316047e-06  3.85405008e-02  4.01382896e-02 -7.05827299e-06\n   4.01390814e-02  4.16318494e-02 -6.81668127e-06  4.16326708e-02\n   4.30160656e-02 -6.50101058e-06  4.30169142e-02  4.42868690e-02\n  -6.11469158e-06  4.42877427e-02  4.54404891e-02 -5.66192239e-06\n   4.54413854e-02  4.64734636e-02 -5.14762318e-06  4.64743805e-02\n   4.73826504e-02 -4.57738326e-06  4.73835851e-02  4.81652351e-02\n  -3.95739894e-06  4.81661852e-02  4.88187408e-02 -3.29440795e-06\n   4.88197039e-02  4.93410359e-02 -2.59572613e-06  4.93420093e-02\n   4.97303401e-02 -1.86872524e-06  4.97313212e-02  4.99852306e-02\n  -1.12141673e-06  4.99862166e-02  5.01046466e-02 -3.61921542e-07\n   5.01056350e-02  5.00878940e-02  4.01506472e-07  5.00888820e-02\n   4.99346475e-02  1.16057155e-06  4.99356323e-02  4.96449525e-02\n   1.90702435e-06  4.96459316e-02  4.92192264e-02  2.63264240e-06\n   4.92201973e-02  4.86582589e-02  3.32976103e-06  4.86592186e-02\n   4.79632093e-02  3.99069350e-06  4.79641553e-02  4.71356059e-02\n   4.60825747e-06  4.71365355e-02  4.61773423e-02  5.17574188e-06\n   4.61782529e-02  4.50906728e-02  5.68709069e-06  4.50915619e-02\n   4.38782078e-02  6.13630416e-06  4.38790731e-02  4.25429080e-02\n   6.51827635e-06  4.25437469e-02  4.10880761e-02  6.82962465e-06\n   4.10888862e-02  3.95173496e-02  7.07495374e-06  3.95181287e-02\n   3.78346920e-02  7.25612172e-06  3.78354378e-02  3.60443819e-02\n   7.30243426e-06  3.60450924e-02  3.41510035e-02  7.02947379e-06\n   3.41516766e-02  3.21594337e-02  6.76328867e-06  3.21600675e-02\n   3.00748305e-02  8.99710093e-06  3.00754231e-02  2.79026193e-02\n   1.54464826e-05  2.79031689e-02  2.56484785e-02  4.51302959e-06\n   2.56489839e-02  2.33183248e-02 -8.42146747e-05  2.33187857e-02\n   2.09183003e-02 -1.54648668e-04  2.09187151e-02  1.84547672e-02\n   5.92038028e-04  1.84551200e-02  1.59342760e-02  2.78866566e-03\n   1.59345385e-02  1.33634130e-02 -3.20583830e-04  1.33636820e-02\n   1.07486597e-02 -2.86018199e-02  1.07494170e-02  8.09754399e-03\n  -5.39480110e-02  8.09874689e-03  5.42219474e-03  1.79342417e-01\n   5.41879347e-03  2.73447683e-03  9.00433216e-01  2.71677762e-03]]\n5.156046648996679e-05\n"
    }
   ],
   "source": [
    "# Error for whole simulation\n",
    "from numpy import linalg as LA\n",
    "\n",
    "FE = 1e-6\n",
    "yseq = sim.compute(sim.p)\n",
    "p = sim.p\n",
    "p = torch.tensor(p, requires_grad = True)\n",
    "y, p0 = Simulate(p)\n",
    "test = sum(y)\n",
    "test.backward()\n",
    "dy_dp = p.grad\n",
    "print(p.grad)\n",
    "dy_dp_FD = np.zeros((1,len(sim.p)))\n",
    "\n",
    "for i in range(len(sim.p)):\n",
    "    dp= np.zeros(len(sim.p))\n",
    "    dp[i] = FE\n",
    "    dp = torch.tensor(dp)\n",
    "    y_p, pp = Simulate(p + dp)\n",
    "    y_m, pm = Simulate(p - dp)\n",
    "    y_p = y_p.detach().numpy()\n",
    "    y_m = y_m.detach().numpy()\n",
    "    dy_dp_FD[0, i] = sum((y_p - y_m) / (2* FE))\n",
    "\n",
    "print(dy_dp_FD)\n",
    "\n",
    "#dy_dp_FD = dy_dp_FD.detach().numpy()\n",
    "dy_dp = dy_dp.detach().numpy()\n",
    "err = LA.norm(dy_dp_FD - dy_dp)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.0002908451455664775\n"
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "#Calculate dy_dp with FE\n",
    "FE = 1e-6\n",
    "dy_dp = sim.dy_dp(sim.y, sim.ydot, sim.yddot, sim.p)\n",
    "dy_dp_FD = np.zeros((len(sim.p),len(sim.p)))\n",
    "\n",
    "for i in range(len(sim.p)):\n",
    "    dp= np.zeros(len(sim.p))\n",
    "    dp[i] = FE\n",
    "    pos = sim.p + dp\n",
    "    y_p = sim.compute(pos)\n",
    "    y_m = sim.compute(sim.p - dp)\n",
    "    dy_dp_FD[:, i] = (y_p - y_m) / (2* FE)\n",
    "err = LA.norm(dy_dp_FD - dy_dp)\n",
    "print(err)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-7017e70665cd>, line 10)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-7017e70665cd>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    print(test), nondet_to, nondet_tol=0.0)\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradcheck takes a tuple of tensors as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "p = sim.p\n",
    "p = torch.tensor(p, requires_grad = True)\n",
    "input = p.double()\n",
    "test = gradcheck(Simulate, input, eps=1e-6, atol=1e-2)\n",
    "print(test), nondet_to, nondet_tol=0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}