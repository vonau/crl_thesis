{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import time\n",
    "import pydde as d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "samplenum = 10\n",
    "epochs = 100\n",
    "hiddenlayers = [90]\n",
    "input_size = 3\n",
    "output_size = 3\n",
    "learning_rate = 0.001\n",
    "time_length = 60; #seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulation\n",
    "dyn = d.PyDyn('test2.sim', time_length)\n",
    "state_init = dyn.compute(dyn.p_init)\n",
    "f = dyn.f(state_init, dyn.p_init)\n",
    "df = dyn.df_dp(state_init, dyn.p_init)\n",
    "dy = dyn.dy_dp(state_init, dyn.p_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample targets only variables in z direction\n",
    "y_target = np.zeros((samplenum,3))\n",
    "y_target[:,2] = np.random.rand(samplenum)\n",
    "#x[:,0] = np.random.rand(samplenum)\n",
    "y_target[:,1] = 2\n",
    "y_target= torch.tensor(y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the custon Simulation Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulate(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        #print(f'input: {input.shape}')\n",
    "        p = input.clone().numpy().transpose()\n",
    "        state = dyn.compute(p)\n",
    "        y_pred = torch.tensor(state.y[-3:])\n",
    "        #print(f'y_pred: {y_pred.dtype}')\n",
    "        \n",
    "        ctx.save_for_backward(input)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        #print(grad_output.shape)\n",
    "        input, = ctx.saved_tensors\n",
    "        p = input.clone().numpy().transpose()\n",
    "        state= dyn.compute(p)\n",
    "        dy_dp = dyn.dy_dp(state, p)\n",
    "        dy_dp = dy_dp[-3:, :]\n",
    "        #print(f'shape of dy/dp: {dy_dp.shape}')\n",
    "        #print(f'shape of grad_output: {grad_output.shape}')\n",
    "        grad_output = grad_output.unsqueeze(0)\n",
    "        \n",
    "        grad_input = torch.tensor(dy_dp).t().mm(grad_output.t()).t()\n",
    "        return grad_input\n",
    "\n",
    "Simulate = Simulate.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveLearn(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, out_sz):\n",
    "        super(ActiveLearn, self).__init__()\n",
    "\n",
    "        self.L_in = nn.Linear(n_in, hiddenlayers[0])\n",
    "        self.H1 = nn.Linear(hiddenlayers[0], 3*time_length)\n",
    "        #self.H1 = nn.Linear(hiddenlayers[0], hiddenlayers[1])\n",
    "        #self.H2 = nn.Linear(hiddenlayers[1], 3*time_length)\n",
    "        self.P = nn.Linear(3*time_length, 3*time_length)\n",
    "        self.Relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.L_in(input)\n",
    "        x = self.Relu(x)\n",
    "        x = self.H1(x)\n",
    "        x = self.Relu(x)\n",
    "        #x = self.H2(x)\n",
    "        #x = self.Relu(x)\n",
    "        x = self.P(x)\n",
    "        x = self.Relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ActiveLearn(input_size, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()  # RMSE = np.sqrt(MSE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "y_target = y_target.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch:   0/100  loss: 0.04818699\nepoch:   1/100  loss: 0.01876822\nepoch:   2/100  loss: 0.02525770\nepoch:   3/100  loss: 0.01432685\nepoch:   4/100  loss: 0.01568288\nepoch:   5/100  loss: 0.01363829\nepoch:   6/100  loss: 0.01301422\nepoch:   7/100  loss: 0.01146603\nepoch:   8/100  loss: 0.01235490\nepoch:   9/100  loss: 0.01256030\nepoch:  10/100  loss: 0.01293924\nepoch:  11/100  loss: 0.01377768\nepoch:  12/100  loss: 0.02032741\nepoch:  13/100  loss: 0.01229425\nepoch:  14/100  loss: 0.01037918\nepoch:  15/100  loss: 0.01485858\nepoch:  16/100  loss: 0.01882355\nepoch:  17/100  loss: 0.01597444\nepoch:  18/100  loss: 0.01202891\nepoch:  19/100  loss: 0.01327722\nepoch:  20/100  loss: 0.01909305\nepoch:  21/100  loss: 0.01484646\nepoch:  22/100  loss: 0.01606056\nepoch:  23/100  loss: 0.01034472\nepoch:  24/100  loss: 0.00953584\nepoch:  25/100  loss: 0.00967595\nepoch:  26/100  loss: 0.01608574\nepoch:  27/100  loss: 0.02550515\nepoch:  28/100  loss: 0.01566953\nepoch:  29/100  loss: 0.02959391\nepoch:  30/100  loss: 0.01184078\nepoch:  31/100  loss: 0.00832690\nepoch:  32/100  loss: 0.01234241\nepoch:  33/100  loss: 0.02344702\nepoch:  34/100  loss: 0.02051462\nepoch:  35/100  loss: 0.01426176\nepoch:  36/100  loss: 0.01120566\nepoch:  37/100  loss: 0.01628351\nepoch:  38/100  loss: 0.01178552\nepoch:  39/100  loss: 0.01813051\nepoch:  40/100  loss: 0.01205818\nepoch:  41/100  loss: 0.01635593\nepoch:  42/100  loss: 0.01273104\nepoch:  43/100  loss: 0.01609490\nepoch:  44/100  loss: 0.01237072\nepoch:  45/100  loss: 0.01552305\nepoch:  46/100  loss: 0.01257818\nepoch:  47/100  loss: 0.01522190\nepoch:  48/100  loss: 0.01305017\nepoch:  49/100  loss: 0.01360201\nepoch:  50/100  loss: 0.01034753\nepoch:  51/100  loss: 0.01278080\nepoch:  52/100  loss: 0.01230364\nepoch:  53/100  loss: 0.01356096\nepoch:  54/100  loss: 0.01118707\nepoch:  55/100  loss: 0.01308008\nepoch:  56/100  loss: 0.01177937\nepoch:  57/100  loss: 0.01287478\nepoch:  58/100  loss: 0.01052532\nepoch:  59/100  loss: 0.01201710\nepoch:  60/100  loss: 0.01052124\nepoch:  61/100  loss: 0.01161296\nepoch:  62/100  loss: 0.00942212\nepoch:  63/100  loss: 0.01074499\nepoch:  64/100  loss: 0.00907291\nepoch:  65/100  loss: 0.01008001\nepoch:  66/100  loss: 0.00800973\nepoch:  67/100  loss: 0.00929011\nepoch:  68/100  loss: 0.00763738\nepoch:  69/100  loss: 0.00788269\nepoch:  70/100  loss: 0.00550346\nepoch:  71/100  loss: 0.01676829\nepoch:  72/100  loss: 0.00536920\nepoch:  73/100  loss: 0.00391671\nepoch:  74/100  loss: 0.01089813\nepoch:  75/100  loss: 0.00881764\nepoch:  76/100  loss: 0.00435007\nepoch:  77/100  loss: 0.01145815\nepoch:  78/100  loss: 0.01670404\nepoch:  79/100  loss: 0.00818283\nepoch:  80/100  loss: 0.00427198\nepoch:  81/100  loss: 0.01992179\nepoch:  82/100  loss: 0.00447699\nepoch:  83/100  loss: 0.00393115\nepoch:  84/100  loss: 0.01372860\nepoch:  85/100  loss: 0.01783388\nepoch:  86/100  loss: 0.01204539\nepoch:  87/100  loss: 0.00526001\nepoch:  88/100  loss: 0.00901956\nepoch:  89/100  loss: 0.01096492\nepoch:  90/100  loss: 0.00922061\nepoch:  91/100  loss: 0.00733915\nepoch:  92/100  loss: 0.00951036\nepoch:  93/100  loss: 0.00896740\nepoch:  94/100  loss: 0.00882590\nepoch:  95/100  loss: 0.00758428\nepoch:  96/100  loss: 0.00890120\nepoch:  97/100  loss: 0.00784831\nepoch:  98/100  loss: 0.00831895\nepoch:  99/100  loss: 0.00707268\nepoch: 100 loss: 0.00707268\n\nDuration: 8.888 min\n"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "losses = []\n",
    "y_preds= np.zeros((samplenum, 3))\n",
    "p_preds= np.zeros((samplenum, 3*time_length))\n",
    "\n",
    "#y_pred = torch.tensor(y_pred)\n",
    "for i in range(epochs):\n",
    "    for s in range(samplenum):\n",
    "        y_truth = y_target[s, :]\n",
    "        #print(y_truth.shape)\n",
    "        #y_truth = y_truth.unsqueeze(0)\n",
    "        p_pred = model(y_truth)\n",
    "        y_pred = Simulate(p_pred)\n",
    "        #print(y_pred.shape)\n",
    "        y_preds[s, :] = y_pred.detach()\n",
    "        p_preds[s, :] = p_pred.detach()\n",
    "        #loss = torch.sqrt(criterion(y_pred.float(), y_truth)) # RMSE\n",
    "        loss = criterion(y_pred.float(), y_truth)  # MSE\n",
    "        #loss =sum((y_pred.float()-y_truth)**2) \n",
    "        losses.append(loss)\n",
    "        optimizer.zero_grad()\n",
    "        #Back Prop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch: {i:3}/{epochs}  loss: {loss.item():10.8f}')\n",
    "    i+=1\n",
    "\n",
    "print(f'epoch: {i:3} loss: {loss.item():10.8f}') # print the last line\n",
    "print(f'\\nDuration: {(time.time() - start_time)/60:.3f} min') # print the time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model saved\n"
    }
   ],
   "source": [
    "#Save Model\n",
    "\n",
    "if len(losses) == epochs*(samplenum):\n",
    "    torch.save(model.state_dict(), 'Trained_Model_300420_300s_100e_onlyZpos.pt')\n",
    "    print('Model saved')\n",
    "else:\n",
    "    print('Model has not been trained. Consider loading a trained model instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.0944146 1.9530724 0.3841143]\ntensor([0.0000, 2.0000, 0.5000])\n[0.0944146 1.9530724 0.3841143]\n0.0\n[0.         0.         0.         0.         0.         0.20432281\n 0.         0.         1.451682   1.3087382  0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         1.6551723  0.         0.         0.\n 0.         0.         0.07099061 0.         0.         0.\n 0.         0.         0.         0.25406152 0.         0.\n 0.         0.         0.         1.1448839  1.0023524  0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.07773931 0.         0.         0.         0.1156168\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.5394917  0.         0.40568525 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.01849897 0.         0.26445943 0.         0.\n 0.         0.         0.         0.         0.         1.8941786\n 0.         0.         0.         0.         0.         0.\n 0.5330909  0.         0.         0.         0.         0.\n 1.5261192  0.         0.         0.         0.         0.71748024\n 0.         0.25342453 0.         0.47286656 0.         1.4034631\n 0.         1.133931   0.         0.16180235 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.3257668  0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.38985682 0.59257495 0.         0.         0.\n 0.9095051  0.7258321  0.         0.         0.         0.\n 0.         0.         0.52319086 0.         0.         0.\n 0.         0.         0.         0.         0.         0.36893564\n 0.         0.6136531  0.         0.         0.6773176  0.\n 0.         0.         1.9948217  0.         0.         0.\n 0.32203013 0.5084514  0.         0.         3.027595   0.        ]\n"
    }
   ],
   "source": [
    "y_target= torch.tensor([0, 2, 0.5])\n",
    "p = model(y_target)\n",
    "y_pred = Simulate(p)\n",
    "y_pred = y_pred.detach().numpy()\n",
    "p = p.detach().numpy()\n",
    "\n",
    "\n",
    "yTraj_test = dyn.compute(p)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_target)\n",
    "print(yTraj_test.y[-3:])\n",
    "print(np.sum(yTraj_test.y[-3:]-y_pred))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Script Conversion and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1.9574, 0.0000, 0.0000, 2.7701, 0.0000], grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "input_example = torch.tensor(y_target[0,:])\n",
    "traced_script_module = torch.jit.trace(model, input_example)\n",
    "original = model(test_input)\n",
    "\n",
    "# Test the torch script\n",
    "test_input= torch.tensor([0, 2, 0.5])\n",
    "output_example = traced_script_module(test_input)\n",
    "print(output_example[-12:])\n",
    "print(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save serialized model\n",
    "traced_script_module.save(\"CPP_example_model_latest.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bitc9de6dffaec048baa4256f94fcb6712f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}