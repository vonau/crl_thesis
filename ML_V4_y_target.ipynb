{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import time\n",
    "import pydde as d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "samplenum = 50\n",
    "epochs = 100\n",
    "hiddenlayers = [90]\n",
    "input_size = 3\n",
    "output_size = 3\n",
    "learning_rate = 0.001\n",
    "time_length = 60; #seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulation\n",
    "dyn = d.PyDyn('Data/point-mass_pendulum.sim', time_length)\n",
    "state_init = dyn.compute(dyn.p_init)\n",
    "f = dyn.f(state_init, dyn.p_init)\n",
    "df = dyn.df_dp(state_init, dyn.p_init)\n",
    "dy = dyn.dy_dp(state_init, dyn.p_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample targets only variables in z direction\n",
    "y_target = np.zeros((samplenum,3))\n",
    "y_target[:,2] = np.random.rand(samplenum)\n",
    "#x[:,0] = np.random.rand(samplenum)\n",
    "y_target[:,1] = 2\n",
    "y_target= torch.tensor(y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the custon Simulation Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulate(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        #print(f'input: {input.shape}')\n",
    "        p = input.clone().numpy().transpose()\n",
    "        state = dyn.compute(p)\n",
    "        y_pred = torch.tensor(state.y[-3:])\n",
    "        #print(f'y_pred: {y_pred.dtype}')\n",
    "        \n",
    "        ctx.save_for_backward(input)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        #print(grad_output.shape)\n",
    "        input, = ctx.saved_tensors\n",
    "        p = input.clone().numpy().transpose()\n",
    "        state= dyn.compute(p)\n",
    "        dy_dp = dyn.dy_dp(state, p)\n",
    "        dy_dp = dy_dp[-3:, :]\n",
    "        #print(f'shape of dy/dp: {dy_dp.shape}')\n",
    "        #print(f'shape of grad_output: {grad_output.shape}')\n",
    "        grad_output = grad_output.unsqueeze(0)\n",
    "        \n",
    "        grad_input = torch.tensor(dy_dp).t().mm(grad_output.t()).t()\n",
    "        return grad_input\n",
    "\n",
    "Simulate = Simulate.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveLearn(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, out_sz):\n",
    "        super(ActiveLearn, self).__init__()\n",
    "\n",
    "        self.L_in = nn.Linear(n_in, hiddenlayers[0])\n",
    "        self.H1 = nn.Linear(hiddenlayers[0], 3*time_length)\n",
    "        #self.H1 = nn.Linear(hiddenlayers[0], hiddenlayers[1])\n",
    "        #self.H2 = nn.Linear(hiddenlayers[1], 3*time_length)\n",
    "        self.P = nn.Linear(3*time_length, 3*time_length)\n",
    "        self.Relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.L_in(input)\n",
    "        x = self.Relu(x)\n",
    "        x = self.H1(x)\n",
    "        x = self.Relu(x)\n",
    "        #x = self.H2(x)\n",
    "        #x = self.Relu(x)\n",
    "        x = self.P(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ActiveLearn(input_size, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()  # RMSE = np.sqrt(MSE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "y_target = y_target.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch:   0/100  loss: 2.27975308\nepoch:   1/100  loss: 1.98562084\nepoch:   2/100  loss: 1.53118231\nepoch:   3/100  loss: 0.73342367\nepoch:   4/100  loss: 0.48224390\nepoch:   5/100  loss: 0.39723121\nepoch:   6/100  loss: 0.39132231\nepoch:   7/100  loss: 0.38500593\nepoch:   8/100  loss: 0.37829823\nepoch:   9/100  loss: 0.37139875\nepoch:  10/100  loss: 0.36428706\nepoch:  11/100  loss: 0.35697664\nepoch:  12/100  loss: 0.34947179\nepoch:  13/100  loss: 0.34172603\nepoch:  14/100  loss: 0.33357242\nepoch:  15/100  loss: 0.30452962\nepoch:  16/100  loss: 0.14911450\nepoch:  17/100  loss: 0.01814017\nepoch:  18/100  loss: 0.00373714\nepoch:  19/100  loss: 0.00085121\nepoch:  20/100  loss: 0.00039413\nepoch:  21/100  loss: 0.00073770\nepoch:  22/100  loss: 0.00283775\nepoch:  23/100  loss: 0.00163454\nepoch:  24/100  loss: 0.00212314\nepoch:  25/100  loss: 0.00104888\nepoch:  26/100  loss: 0.00087157\nepoch:  27/100  loss: 0.00082102\nepoch:  28/100  loss: 0.00083013\nepoch:  29/100  loss: 0.00093871\nepoch:  30/100  loss: 0.00080241\nepoch:  31/100  loss: 0.00082100\nepoch:  32/100  loss: 0.00080769\nepoch:  33/100  loss: 0.00076873\nepoch:  34/100  loss: 0.00074144\nepoch:  35/100  loss: 0.00071224\nepoch:  36/100  loss: 0.00068221\nepoch:  37/100  loss: 0.00065162\nepoch:  38/100  loss: 0.00062068\nepoch:  39/100  loss: 0.00058963\nepoch:  40/100  loss: 0.00055869\nepoch:  41/100  loss: 0.00052809\nepoch:  42/100  loss: 0.00049802\nepoch:  43/100  loss: 0.00046862\nepoch:  44/100  loss: 0.00044003\nepoch:  45/100  loss: 0.00041235\nepoch:  46/100  loss: 0.00038568\nepoch:  47/100  loss: 0.00036001\nepoch:  48/100  loss: 0.00033540\nepoch:  49/100  loss: 0.00031188\nepoch:  50/100  loss: 0.00028944\nepoch:  51/100  loss: 0.00026810\nepoch:  52/100  loss: 0.00024788\nepoch:  53/100  loss: 0.00022878\nepoch:  54/100  loss: 0.00021082\nepoch:  55/100  loss: 0.00019403\nepoch:  56/100  loss: 0.00017843\nepoch:  57/100  loss: 0.00016405\nepoch:  58/100  loss: 0.00015092\nepoch:  59/100  loss: 0.00013904\nepoch:  60/100  loss: 0.00012842\nepoch:  61/100  loss: 0.00011906\nepoch:  62/100  loss: 0.00011090\nepoch:  63/100  loss: 0.00010389\nepoch:  64/100  loss: 0.00009796\nepoch:  65/100  loss: 0.00009302\nepoch:  66/100  loss: 0.00008897\nepoch:  67/100  loss: 0.00008574\nepoch:  68/100  loss: 0.00008326\nepoch:  69/100  loss: 0.00008147\nepoch:  70/100  loss: 0.00008035\nepoch:  71/100  loss: 0.00007986\nepoch:  72/100  loss: 0.00008000\nepoch:  73/100  loss: 0.00008072\nepoch:  74/100  loss: 0.00008197\nepoch:  75/100  loss: 0.00008361\nepoch:  76/100  loss: 0.00008534\nepoch:  77/100  loss: 0.00008777\nepoch:  78/100  loss: 0.00009042\nepoch:  79/100  loss: 0.00009319\nepoch:  80/100  loss: 0.00009602\nepoch:  81/100  loss: 0.00009883\nepoch:  82/100  loss: 0.00010155\nepoch:  83/100  loss: 0.00010411\nepoch:  84/100  loss: 0.00010646\nepoch:  85/100  loss: 0.00010857\nepoch:  86/100  loss: 0.00011047\nepoch:  87/100  loss: 0.00011226\nepoch:  88/100  loss: 0.00011404\nepoch:  89/100  loss: 0.00011597\nepoch:  90/100  loss: 0.00011815\nepoch:  91/100  loss: 0.00012072\nepoch:  92/100  loss: 0.00012374\nepoch:  93/100  loss: 0.00012723\nepoch:  94/100  loss: 0.00013121\nepoch:  95/100  loss: 0.00013555\nepoch:  96/100  loss: 0.00014015\nepoch:  97/100  loss: 0.00014484\nepoch:  98/100  loss: 0.00014942\nepoch:  99/100  loss: 0.00015363\nepoch: 100 loss: 0.00015363\n\nDuration: 41.194 min\n"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "start_time = time.time()\n",
    "weight_c1 = 1 # p start condition\n",
    "weight_c2 = 1 # y start condition\n",
    "losses = []\n",
    "y_preds= np.zeros((samplenum, 3))\n",
    "p_preds= np.zeros((samplenum, 3*time_length))\n",
    "\n",
    "#y_pred = torch.tensor(y_pred)\n",
    "for i in range(epochs):\n",
    "    for s in range(samplenum):\n",
    "        y_truth = y_target[s, :]\n",
    "        p_pred = model(y_truth)\n",
    "        y_pred = Simulate(p_pred)\n",
    "        y_preds[s, :] = y_pred.detach()\n",
    "        p_preds[s, :] = p_pred.detach()\n",
    "        smoothness_error = criterion(p_pred[0:3*(time_length-1)], p_pred[3:3*time_length])\n",
    "        #loss = torch.sqrt(criterion(y_pred.float(), y_truth)) # RMSE\n",
    "        #loss = criterion(y_pred.float(), y_truth) + weight_c1*(sum(p_pred[0:3]-torch.tensor(dyn.p_init[0:3])))**2  # MSE + start condition penalty + p smoothness condition penalty\n",
    "        loss = criterion(y_pred.float(), y_truth) + weight_c1*criterion(p_pred[0:3].double(), torch.tensor(dyn.p_init[0:3])) + weight_c2*smoothness_error# MSE + start condition penalty + p smoothness condition penalty\n",
    "        #loss =sum((y_pred.float()-y_truth)**2) \n",
    "        losses.append(loss)\n",
    "        optimizer.zero_grad()\n",
    "        #Back Prop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch: {i:3}/{epochs}  loss: {loss.item():10.8f}')\n",
    "    i+=1\n",
    "\n",
    "print(f'epoch: {i:3} loss: {loss.item():10.8f}') # print the last line\n",
    "print(f'\\nDuration: {(time.time() - start_time)/60:.3f} min') # print the time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model saved\n"
    }
   ],
   "source": [
    "#Save Model\n",
    "\n",
    "if len(losses) == epochs*(samplenum):\n",
    "    torch.save(model.state_dict(), 'Trained_Model_300420_300s_100e_onlyZpos.pt')\n",
    "    print('Model saved')\n",
    "else:\n",
    "    print('Model has not been trained. Consider loading a trained model instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.00401509 1.98388322 0.48141816]\ntensor([0.0000, 2.0000, 0.5000])\np trajecory: [-4.21194732e-03  2.99492264e+00 -1.17538869e-03  1.41971314e+00\n -3.83457631e-01  5.21476090e-01 -3.77948165e-01  5.39184034e-01\n  6.50334239e-01 -8.88038278e-01  6.70214534e-01 -6.37364745e-01\n  6.04890585e-01 -6.59534156e-01 -1.19655691e-01  9.54622030e-01\n  7.08152413e-01 -7.03089595e-01 -1.87960416e-01  5.71996160e-02\n  1.26621199e+00 -5.28571248e-01 -7.13740051e-01 -9.47222352e-01\n -2.33718544e-01  8.46694931e-02 -1.43422306e-01 -7.10699737e-01\n -7.43213415e-01  3.23251545e-01 -5.05875647e-02  1.52284637e-01\n  1.48828757e+00  1.09069109e+00 -1.09838033e+00 -2.23105535e-01\n  6.53168917e-01  5.32971740e-01 -2.53608555e-01  2.20701218e-01\n  1.01644725e-01  4.31829989e-01  8.27530384e-01  1.10764563e+00\n -9.38422978e-01 -1.37930006e-01  6.48558974e-01  3.64954211e-02\n -8.99044335e-01 -4.41821814e-01 -1.04966390e+00 -9.82296407e-01\n -9.26035643e-01  1.53821409e+00  9.41120625e-01  1.19896913e+00\n -7.49491900e-02 -4.69397992e-01 -1.70810997e-01  7.69999623e-01\n -9.68683302e-01 -2.05960184e-01 -3.64899784e-01  6.50870502e-01\n -2.58231372e-01  2.54275978e-01 -3.32585812e-01  3.33287776e-01\n  1.48593354e+00  4.26078707e-01 -1.21228647e+00 -6.22197866e-01\n  3.40572119e-01  4.28926766e-01  2.73297220e-01 -4.66879308e-01\n  2.93326586e-01  1.59641296e-01  5.07365942e-01 -7.18593836e-01\n  3.98707032e-01  3.52480203e-01  1.13052726e+00 -1.67500690e-01\n  4.70093280e-01  7.71974772e-02  6.44990146e-01  2.47805834e-01\n  5.85735023e-01  4.77761894e-01 -1.27606630e-01 -2.21129909e-01\n -3.71229738e-01 -5.31406581e-01 -1.19706166e+00  1.33977449e+00\n -5.49573183e-01  5.76622844e-01 -3.19350719e-01 -1.19245425e-02\n -1.84303567e-01  6.31225765e-01 -2.64598250e-01 -9.46465075e-01\n -1.14293896e-01  5.72314203e-01 -7.14010358e-01  1.74288809e-01\n  5.71953475e-01  7.33233571e-01  3.62292796e-01 -6.43647984e-02\n -1.24586836e-01  8.68792713e-01 -2.95051366e-01 -4.95760679e-01\n -9.08835471e-01  1.61484748e-01 -1.26400873e-01  3.39318156e-01\n -2.38641888e-01 -7.19984353e-01 -7.70560563e-01 -4.58335370e-01\n  5.49372435e-01 -1.06402516e-01  5.47822595e-01  4.65506732e-01\n -1.13609806e-01 -1.30663529e-01  3.49854231e-01  4.34443384e-01\n -5.35793975e-03 -2.67056286e-01  3.84313352e-02 -3.80319133e-02\n  1.95330530e-01 -2.90366411e-01  6.17954195e-01 -1.43456185e+00\n  8.37438166e-01 -1.69673398e-01 -4.09970507e-02 -2.73900330e-01\n  7.27431357e-01 -7.99557567e-01 -8.90058279e-01 -9.44292963e-01\n -1.72999129e-02  1.39337564e+00  9.15329337e-01  2.80524343e-01\n -4.00305301e-01  4.74092633e-01 -8.38324726e-01  9.76010263e-01\n -2.35928923e-01  4.01067048e-01  7.16351748e-01  5.26835978e-01\n  4.81391847e-01  1.07459679e-01  7.01847196e-01 -4.28299189e-01\n  1.47122759e-02 -6.61150694e-01  1.32574666e+00 -2.04861075e-01\n  8.26300830e-02 -3.20279062e-01 -7.68725574e-01  3.60002518e-01\n  5.33087291e-02  5.72818518e-03  5.87145865e-01  1.10693121e+00\n  1.97029853e+00 -4.23230797e-01  2.42546439e+00 -5.52702665e-01]\ny trajectory: [ 2.93909939e-05  1.99513582e+00  2.16440186e-06  8.39677279e-01\n  5.85384974e-01  3.08418769e-01  5.58825418e-01 -8.29453735e-02\n  6.35080626e-01 -3.51329722e-01  1.46564250e-02  9.86951349e-02\n -3.92610696e-01 -2.48499121e-01 -2.89632201e-01  1.46657661e-01\n -3.22865658e-03 -6.88466627e-01  2.03690517e-01  1.38819338e-01\n  2.11434682e-01 -1.62649070e-01 -2.53896852e-01  7.13883902e-03\n  3.97969450e-02  7.64597541e-01 -9.36466194e-02 -3.13622435e-01\n  3.08325654e-01  1.07540342e-01 -5.17894353e-01 -7.73728511e-02\n  5.94114937e-01  2.20873963e-01 -7.94863689e-01  4.02461211e-01\n  8.11779716e-01 -5.26967188e-01 -1.32568376e-02  1.04238198e+00\n -1.51008384e-01 -1.66576297e-01  1.21506421e+00  3.37351979e-01\n -4.00319615e-01  8.29057293e-01  7.59113225e-01 -3.88510428e-01\n -2.27578164e-01  3.68669357e-01 -7.12972566e-01 -1.10814713e+00\n -5.50240647e-01  4.64584433e-01  9.75159239e-02  4.29937464e-01\n  4.19462565e-01  3.55662983e-01  5.63776300e-01  5.85836823e-01\n -1.54247807e-01  2.57665447e-01  2.10025061e-01 -3.26532260e-01\n -1.04332145e-01 -5.79379672e-02 -4.23522047e-01 -1.05631865e-01\n  4.94773594e-01 -4.66409315e-02 -6.61642050e-01  2.11628292e-01\n  3.34055139e-01 -6.11389540e-01  5.58019215e-02  3.66156304e-01\n -3.10981303e-01 -2.34199990e-02  3.83889301e-01  7.95802225e-02\n -1.68663062e-01  4.22254499e-01  1.89011253e-01 -3.75374126e-01\n  4.62311811e-01  2.56686591e-01 -3.62912381e-01  5.26168095e-01\n  2.96946554e-01 -4.27879739e-01  6.49436511e-01  3.80480169e-01\n -5.02915706e-01 -4.55911575e-03 -5.27105353e-01  5.65033095e-01\n -5.94171525e-01 -2.47578857e-01  4.79599096e-01 -9.96865240e-01\n -4.83608848e-03  4.31977714e-01 -9.70486811e-01 -2.11540135e-01\n  1.95841504e-01 -4.50775025e-01 -5.16303417e-01  2.95151262e-02\n  2.58796720e-01 -2.36103963e-01  5.15719003e-02  7.45256644e-01\n  5.51704933e-03  2.45416011e-01  4.81223677e-01 -1.19424243e-01\n -2.23405195e-01  2.15257153e-01 -2.42901389e-01 -6.56421418e-01\n  1.10498150e-01 -6.81198202e-02 -1.36296872e+00 -2.12588309e-01\n  3.13495422e-01 -1.14595404e+00 -2.68339361e-01  6.36406966e-01\n -7.27770017e-01 -3.27761292e-01  9.68108281e-01 -3.23706272e-01\n -2.55813087e-01  7.58929033e-01  6.59324399e-02 -2.09279531e-01\n  6.08315359e-01  5.85567422e-01  2.03876997e-01 -4.32365550e-01\n  9.79458201e-01  2.07661636e-01 -7.29170290e-01  5.16184080e-01\n  2.53378598e-01 -1.01007841e+00 -2.37218059e-02 -4.09938955e-01\n -5.65845806e-01  5.52341194e-01  3.10434692e-02  1.00489760e-01\n  2.79482401e-01  4.72902508e-01  8.44260168e-02  4.17792421e-01\n  7.34958381e-01  1.18050444e-01  5.81142478e-01  1.06650964e+00\n  9.98128012e-02  8.38650102e-01  1.22371191e+00 -4.81524738e-02\n  8.25337268e-01  1.90503717e-01  6.89908244e-01  2.19236630e-01\n -4.18480510e-01  6.25000747e-01 -5.61930234e-01 -3.95403738e-01\n  3.27743829e-01 -7.28639456e-01  2.35545053e-01  7.11467379e-01\n  9.20176156e-01  4.01509277e-03  1.98388322e+00  4.81418165e-01]\n3.2723097308729403\n"
    }
   ],
   "source": [
    "y_target= torch.tensor([0, 2, 0.5])\n",
    "p = model(y_target)\n",
    "p = p.detach().numpy()\n",
    "\n",
    "y_pred_state = dyn.compute(p)\n",
    "y_pred = y_pred_state.y[-3:]\n",
    "\n",
    "print(y_pred)\n",
    "print(y_target)\n",
    "print(f'p trajecory: {p}')\n",
    "print(f\"y trajectory: {y_pred_state.y}\")\n",
    "error = 0\n",
    "for i in range(time_length-1):\n",
    "    step = sum((p[3*i:3*i+3] - p[3*i+3:3*i+6])**2)\n",
    "    error = error + step\n",
    "print(error/time_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "p[3*i+3:3*i+6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Script Conversion and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1.9574, 0.0000, 0.0000, 2.7701, 0.0000], grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "input_example = torch.tensor(y_target[0,:])\n",
    "traced_script_module = torch.jit.trace(model, input_example)\n",
    "original = model(test_input)\n",
    "\n",
    "# Test the torch script\n",
    "test_input= torch.tensor([0, 2, 0.5])\n",
    "output_example = traced_script_module(test_input)\n",
    "print(output_example[-12:])\n",
    "print(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save serialized model\n",
    "traced_script_module.save(\"CPP_example_model_latest.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bitc9de6dffaec048baa4256f94fcb6712f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}