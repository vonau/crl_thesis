{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import pydde as d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "samplenum = 3\n",
    "batchsize = samplenum\n",
    "epochs = 10\n",
    "hiddenlayers = [200,100]\n",
    "input_size = 3\n",
    "output_size = 180\n",
    "learning_rate = 0.01\n",
    "time_length = 60; #seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulation\n",
    "sim = d.PySimSeq('test2.sim', 60)\n",
    "yseq = sim.compute(sim.p)\n",
    "f = sim.f(sim.y, sim.ydot, sim.yddot, sim.p)\n",
    "df = sim.df_dp(sim.y, sim.ydot, sim.yddot, sim.p)\n",
    "dy = sim.dy_dp(sim.y, sim.ydot, sim.yddot, sim.p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample targets only variables in z direction\n",
    "x = np.zeros((samplenum,3))\n",
    "x[:,2] = np.random.rand(samplenum)\n",
    "#x[:,0] = np.random.rand(samplenum)\n",
    "x[:,1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duration: 0.092 min\n"
     ]
    }
   ],
   "source": [
    "#Sample ytraj\n",
    "start_time1 = time.time()\n",
    "p = np.zeros((samplenum, 3*time_length))\n",
    "y = np.zeros((samplenum, 3*time_length))\n",
    "\n",
    "for i in range(samplenum):\n",
    "    p[i, :] = sim.sample_ptraj(x[i, :], sim.p)\n",
    "    y[i, :] = sim.compute(p[i, :].transpose())\n",
    "\n",
    "print(f'\\nDuration: {(time.time() - start_time1)/60:.3f} min') # print the time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocesing\n",
    "x= torch.tensor(x)\n",
    "y= torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the custon Simulation Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulate(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, y_target):\n",
    "        #print(input.shape)\n",
    "        #print(y_target.shape)\n",
    "        #ctx.mark_dirty( y_target)\n",
    "        p = input.clone().numpy().transpose()\n",
    "        y_target = y_target.clone().numpy()\n",
    "        #print(p.shape)\n",
    "        #print(y_target.shape)\n",
    "        yTraj_pred = np.zeros((len(y_target[:, 1]),180))\n",
    "        \n",
    "        for i in range(len(y_target[:,1])):\n",
    "            yTraj_pred[i, :] = sim.compute(p[:, i])\n",
    "\n",
    "        yTraj_pred = torch.tensor(yTraj_pred)\n",
    "        #print(f'yTraj_pred {yTraj_pred.shape}')\n",
    "        \n",
    "        ctx.save_for_backward(input, yTraj_pred)\n",
    "        \n",
    "        return yTraj_pred, input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, input):\n",
    "        input, yTraj_pred = ctx.saved_tensors\n",
    "        p = input.clone().numpy().transpose()\n",
    "        yTraj_pred = yTraj_pred.clone().numpy()\n",
    "        dy_dp = np.zeros((samplenum, 3*time_length))\n",
    "        for i in range(len(yTraj_pred[:,1])):\n",
    "            yTraj_pred[i, :] = sim.compute(p[:, i])\n",
    "            dy_dp = sim.dy_dp(sim.y, sim.ydot, sim.yddot, p[:, i])\n",
    "        #print(dy_dp)\n",
    "        \n",
    "        grad_input = torch.tensor(dy_dp).mm(grad_output.t()).t()\n",
    "        return grad_input, None\n",
    "\n",
    "Simulate = Simulate.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveLearn(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, out_sz):\n",
    "        super(ActiveLearn, self).__init__()\n",
    "\n",
    "        self.L_in = nn.Linear(input_size, output_size)\n",
    "        self.P = nn.Linear(output_size, output_size)\n",
    "        self.Relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.L_in(input)\n",
    "        x = self.Relu(x)\n",
    "        x = self.P(x)\n",
    "        x = self.Relu(x)\n",
    "        x, p = Simulate(x, input)\n",
    "        return x, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActiveLearn(input_size, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()  # RMSE = np.sqrt(MSE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "x_train = x.float()\n",
    "y_train = y.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1/10  loss: 1.11863192\n",
      "epoch:  10 loss: 1.19288645\n",
      "\n",
      "Duration: 2.763 min\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "start_time = time.time()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_pred, p_pred = model(x_train)\n",
    "\n",
    "    #loss = torch.sqrt(criterion(y_pred, y)) # RMSE\n",
    "    loss = criterion(y_pred, y) # RMSE\n",
    "    losses.append(loss)\n",
    "\n",
    "    if i%10 == 1:\n",
    "        print(f'epoch: {i:3}/{epochs}  loss: {loss.item():10.8f}')\n",
    "\n",
    "    #Clear the gradient buffer (w <-- w - lr*gradient)\n",
    "    optimizer.zero_grad()\n",
    "    #Back Prop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    i+=1\n",
    "\n",
    "print(f'epoch: {i:3} loss: {loss.item():10.8f}') # print the last line\n",
    "print(f'\\nDuration: {(time.time() - start_time1)/60:.3f} min') # print the time elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradcheck takes a tuple of tensors as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "input = (x_train, y_train)\n",
    "test = gradcheck(Simulate, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
