{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['data_1.json', '.DS_Store', 'data_0.json', 'data_10.json', 'data_7.json', 'data_test', 'cube.out', 'data_6.json', 'data_11.json', 'Description.json', 'data_9.json', 'data_12.json', 'data_5.json', 'data_4.json', 'data_8.json', 'data_13.json', 'data_3.json', 'data_14.json', 'Sampling_cube-drag.py', 'data_2.json']\n"
    }
   ],
   "source": [
    "########################################\n",
    "#LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import time\n",
    "import pydde as dde\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "########################################\n",
    "#PARAMETERS\n",
    "nTimeSteps = 60; #at 60 Hz\n",
    "epochs = 200\n",
    "minibatch_size= 50\n",
    "input_size = 25\n",
    "samplenum_target = 15000\n",
    "samplenum_target_test = 5000\n",
    "hiddenlayers = [100, 200]\n",
    "use_case = 'cube-drag'\n",
    "learning_rate = 0.01\n",
    "LRdecay = 0.7\n",
    "model_file_path = '../Trained_Models/'\n",
    "sample_file_path = f'/Users/nicovonau/Code/thesis_pytorch/Data/Samples/data_{use_case}_{nTimeSteps}tsteps_2315/'\n",
    "simulation_file_path = '/Users/nicovonau/Code/thesis_pytorch/Data/Simulations/cube-drag.sim'\n",
    "objective_file_path = f'/Users/nicovonau/Code/thesis_pytorch/Data/Objectives/cube-drag.obj'\n",
    "# set log level\n",
    "dde.set_log_level(dde.LogLevel.off)\n",
    "print(os.listdir(sample_file_path))\n",
    "\n",
    "dyn = dde.DynamicSequence()\n",
    "dyn.loadFile(simulation_file_path, nTimeSteps)\n",
    "output_size = dyn.nParameters*nTimeSteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Shape of input: (15000, 25)\nShape of p: (15000, 240)\nShape of input after removing faulty samples: (15000, 25)\nShape of p after removing faulty samples: (15000, 240)\n(15000, 25)\n(15000, 240)\n\nShape of input_test: (5000, 25)\nShape of p_test: (5000, 240)\nShape of input_test after removing faulty samples: (5000, 25)\nShape of p_test after removing faulty samples: (5000, 240)\n(5000, 25)\n(5000, 240)\n"
    }
   ],
   "source": [
    "#normalize qddot\n",
    "'''\n",
    "def minmaxscale(input, extrema):\n",
    "    if extrema == None:\n",
    "        maximas= []\n",
    "        minimas= []\n",
    "        for i in range(len(input[0, :])):\n",
    "            maximas.append(np.max(input[:,i]))\n",
    "            minimas.append(np.min(input[:,i]))\n",
    "        max = np.max(maximas)\n",
    "        min = np.min(minimas)\n",
    "        extrema = np.max([max, np.linalg.norm(min)])\n",
    "        scaled = (input+extrema)/(2*extrema)\n",
    "        return scaled, extrema\n",
    "    else:\n",
    "        scaled = (input+extrema)/(2*extrema)\n",
    "        return scaled\n",
    "print(input[0:4, 9:12])\n",
    "data[:, 6:9], extr_qdot = minmaxscale(data[:, 6:9], None)\n",
    "data[:, 9:12], extr_qddot = minmaxscale(data[:, 9:12], None)\n",
    "p_data, extr_p = minmaxscale(p_data, None)\n",
    "print(data[0:4, 9:12])\n",
    "'''\n",
    "#########################################\n",
    "#LOAD TRAINING SAMPLES\n",
    "number_of_files = len(os.listdir(sample_file_path))-5\n",
    "samplenum = 1000*number_of_files\n",
    "output_size = dyn.nParameters*nTimeSteps\n",
    "\n",
    "p = np.zeros((samplenum, dyn.nParameters*nTimeSteps))\n",
    "input = np.zeros((samplenum, input_size))\n",
    "\n",
    "for filenum in range(number_of_files):\n",
    "    with open(sample_file_path + f'data_{filenum}.json') as json_file:\n",
    "        data_ = json.load(json_file)\n",
    "        filesize = len(data_['q_target'])\n",
    "        for i, q_target_i in enumerate(data_['q_target']):\n",
    "            input[filenum*filesize+i, 0:3] = np.array(q_target_i)\n",
    "        for i, q_i in enumerate(data_['q']):\n",
    "            input[filenum*filesize+i, 3:9] = np.array(q_i)\n",
    "        for i, qdot_i in enumerate(data_['qdot']):\n",
    "            input[filenum*filesize+i, 9:15] = np.array(qdot_i)\n",
    "        for i, qddot_i in enumerate(data_['qddot']):\n",
    "            input[filenum*filesize+i, 15:21] = np.array(qddot_i)\n",
    "        for i, p_now_i in enumerate(data_['p_now']):\n",
    "            input[filenum*filesize+i, 21:25] = np.array(p_now_i)\n",
    "        for i, p_i in enumerate(data_['p']):\n",
    "            p[filenum*filesize+i, :] = np.array(p_i)\n",
    "\n",
    "print(f'Shape of input: {input.shape}')\n",
    "print(f'Shape of p: {p.shape}')\n",
    "#Remove zeros\n",
    "p = p[~(input == 0).all(1)]\n",
    "input = input[~(input == 0).all(1)]\n",
    "\n",
    "print(f'Shape of input after removing faulty samples: {input.shape}')\n",
    "print(f'Shape of p after removing faulty samples: {p.shape}')\n",
    "\n",
    "data = input[0:samplenum_target, :]\n",
    "p = p[0:samplenum_target, :]\n",
    "print(data.shape)\n",
    "print(p.shape)\n",
    "\n",
    "data = torch.tensor(data).float()\n",
    "p = torch.tensor(p).float()\n",
    "\n",
    "\n",
    "#########################################\n",
    "#LOAD TEST SAMPLES\n",
    "number_of_files_test = len(os.listdir(sample_file_path + 'data_test/'))\n",
    "samplenum_test = 1000*number_of_files_test\n",
    "\n",
    "p_test = np.zeros((samplenum_test, dyn.nParameters*nTimeSteps))\n",
    "input_test = np.zeros((samplenum_test, input_size))\n",
    "\n",
    "for filenum in range(number_of_files_test):\n",
    "    with open(sample_file_path + f'data_test/data_{filenum}.json') as json_file:\n",
    "        data_ = json.load(json_file)\n",
    "        filesize = len(data_['q_target'])\n",
    "        for i, q_target_i in enumerate(data_['q_target']):\n",
    "            input_test[filenum*filesize+i, 0:3] = np.array(q_target_i)\n",
    "        for i, q_i in enumerate(data_['q']):\n",
    "            input_test[filenum*filesize+i, 3:9] = np.array(q_i)\n",
    "        for i, qdot_i in enumerate(data_['qdot']):\n",
    "            input_test[filenum*filesize+i, 9:15] = np.array(qdot_i)\n",
    "        for i, qddot_i in enumerate(data_['qddot']):\n",
    "            input_test[filenum*filesize+i, 15:21] = np.array(qddot_i)\n",
    "        for i, p_now_i in enumerate(data_['p_now']):\n",
    "            input_test[filenum*filesize+i, 21:25] = np.array(p_now_i)\n",
    "        for i, p_i in enumerate(data_['p']):\n",
    "            p_test[filenum*filesize+i, :] = np.array(p_i)\n",
    "\n",
    "print(f'\\nShape of input_test: {input_test.shape}')\n",
    "print(f'Shape of p_test: {p_test.shape}')\n",
    "#Remove zeros\n",
    "p_test = p_test[~(input_test == 0).all(1)]\n",
    "input_test = input_test[~(input_test == 0).all(1)]\n",
    "\n",
    "print(f'Shape of input_test after removing faulty samples: {input_test.shape}')\n",
    "print(f'Shape of p_test after removing faulty samples: {p_test.shape}')\n",
    "\n",
    "input_test = input_test[0:samplenum_target_test, :]\n",
    "p_test = p_test[0:samplenum_target_test, :]\n",
    "print(input_test.shape)\n",
    "print(p_test.shape)\n",
    "\n",
    "input_test = torch.tensor(input_test).float()\n",
    "p_test = torch.tensor(p_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch:   0/200    LR:   0.010000  loss: 0.01515404\nepoch:  20/200    LR:   0.004000  loss: 0.00647562\nepoch:  40/200    LR:   0.001600  loss: 0.00597326\nepoch:  60/200    LR:   0.000256  loss: 0.00582630\nepoch:  80/200    LR:   0.000102  loss: 0.00581199\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2e849c30d71d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m                         \u001b[0mp_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                         \u001b[0;31m#error claculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                         \u001b[0mloss_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_c1\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msmooth_l1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2164\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2165\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2166\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2167\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mformat_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;34m\"\"\"Shorthand for 'format_list(extract_stack(f, limit))'.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "#Parameters\n",
    "learning_rate = 0.01\n",
    "epochs_s = [200, ]\n",
    "minibatch_size = [50]\n",
    "LRdecay_s = [0.4]\n",
    "hiddenlayers_s = [[10, 50], [30, 100]] \n",
    "\n",
    "\n",
    "#########################################\n",
    "#GRIDSEARCH\n",
    "timestr = time.strftime(\"%m%d%H\")\n",
    "start_time = time.time()\n",
    "scores = []\n",
    "index = 0\n",
    "for e1 in epochs_s:\n",
    "    for b1 in minibatch_size:\n",
    "        for d in LRdecay_s:\n",
    "            for h in hiddenlayers_s:\n",
    "                class ActiveLearn(nn.Module):\n",
    "\n",
    "                    def __init__(self, n_in, out_sz):\n",
    "                        super(ActiveLearn, self).__init__()\n",
    "\n",
    "                        self.L_in = nn.Linear(n_in, h[0])\n",
    "                        self.H1 = nn.Linear(h[0], h[1])\n",
    "                        self.H2 = nn.Linear(h[1], out_sz)\n",
    "                        #self.H3 = nn.Linear(h[2], 3*nTimeSteps)\n",
    "                        self.L_out = nn.Linear(out_sz, out_sz)\n",
    "                        self.Relu = nn.ReLU(inplace=True)\n",
    "                        #self.drop = nn.Dropout(p=0.3)\n",
    "                        #self.norm1 = nn.BatchNorm2d(h[0])\n",
    "                        #self.norm2 = nn.BatchNorm2d(h[1])\n",
    "                    \n",
    "                    def forward(self, input):\n",
    "                        x = self.L_in(input)\n",
    "                        #x = self.norm1(x)\n",
    "                        #x = self.drop(x)\n",
    "                        x = self.Relu(x)\n",
    "                        x = self.H1(x)\n",
    "                        #x = self.norm2(x)\n",
    "                        x = self.Relu(x)\n",
    "                        x = self.H2(x)\n",
    "                        x = self.Relu(x)\n",
    "                        #x = self.H3(x)\n",
    "                        #x = self.Relu(x)\n",
    "                        x = self.L_out(x)\n",
    "                        return x\n",
    "\n",
    "\n",
    "                model = ActiveLearn(input_size, output_size)\n",
    "\n",
    "                criterion = nn.SmoothL1Loss()  # RMSE = np.sqrt(MSE)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "                scheduler= torch.optim.lr_scheduler.StepLR(optimizer, step_size = 15, gamma=d, last_epoch=-1)\n",
    "                #train\n",
    "                torch.autograd.set_detect_anomaly(True)\n",
    "                weight_c1 = 1 # p error condition\n",
    "                batch = np.floor(samplenum/b1).astype(int)\n",
    "                losses= []\n",
    "                p_smoothness_errors = []\n",
    "                basic_errors = [] #y_end_ and p_start error\n",
    "                for e in range(e1):\n",
    "                    for b in range(batch):\n",
    "                        loss_batch = 0\n",
    "                        smoothness_error_batch = 0\n",
    "                        input_i = data[b*b1:b*b1+b1,:]\n",
    "                        p_i = p[b*b1:b*b1+b1,:]\n",
    "                        p_pred = model(input_i)\n",
    "                        #error claculation\n",
    "                        loss_batch = weight_c1* criterion(p_pred, p_i)\n",
    "                        losses.append(loss_batch)\n",
    "                        optimizer.zero_grad()\n",
    "                        #Back Prop\n",
    "                        loss_batch.backward()\n",
    "                        optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    LR= scheduler.get_last_lr()\n",
    "                    if e%(epochs/10) == 0:\n",
    "                        print(f'epoch: {e:3}/{e1}    LR: {LR[0]:10.6f}  loss: {loss_batch.item():10.8f}')\n",
    "                    \n",
    "                print(f'Model {index} trained')\n",
    "                print(f'epoch: {e:3} final loss: {loss.item():10.8f}') # print the last line\n",
    "                print(f'Training completed. Total duration: {(time.time() - start_time)/60:.3f} min') # print the time elapsed\n",
    "                \n",
    "\n",
    "\n",
    "                #Test the data\n",
    "                #model.eval()\n",
    "                losses_test= []\n",
    "                with torch.no_grad():\n",
    "                    for i in range(samplenum_target_test):\n",
    "                        p_val = model(input_test[i, :])\n",
    "                        loss2 = criterion(p_val,p_test[i,:])\n",
    "                        losses_test.append(loss2.clone().numpy())\n",
    "                #plot test errors\n",
    "                plot = plt.plot(losses, label = 'loss', linewidth=3)\n",
    "                plt.legend()\n",
    "                plt.yscale('log')\n",
    "                plt.ylabel('error')\n",
    "                plt.xlabel('batches')\n",
    "                plt.savefig(f'../GridSearch_scores/{index}_Loss_.png')\n",
    "                tot_error = sum(losses_test)\n",
    "                mean_error = np.mean(losses_test)\n",
    "                scores.append([index, tot_error, np.double(mean_error), e1, b1, d, h])\n",
    "                index = index + 1\n",
    "                print(f'TOTAL ERROR: {tot_error}    mean error: {mean_error}  epochs: {e1}    batchsize: {b1}   LRdecay: {d}  hiddenlayer{h}')\n",
    "                print(\"\\nNEXT MODEL\")\n",
    "print(f'\\nDuration: {(time.time() - start_time)/60:.3f} min') # print the time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'Gridsearch_scores_vastart_active_{layerz}2.json', 'w') as outfile:\n",
    "    json.dump(scores, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda39d998b0540f4e659e09acca95c25f2f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}