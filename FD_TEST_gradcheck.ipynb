{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import pydde as d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "samplenum = 1\n",
    "input_size = 3\n",
    "output_size = 3\n",
    "time_length = 60; #seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 2.15365312e-05  1.99969848e+00  0.00000000e+00  2.15757288e-05\n  1.99969848e+00  0.00000000e+00  2.15562034e-05  1.99969848e+00\n  0.00000000e+00  2.14781676e-05  1.99969848e+00  0.00000000e+00\n  2.13419924e-05  1.99969848e+00  0.00000000e+00  2.11482057e-05\n  1.99969848e+00  0.00000000e+00  2.08974905e-05  1.99969848e+00\n  0.00000000e+00  2.05906827e-05  1.99969848e+00  0.00000000e+00\n  2.02287680e-05  1.99969848e+00  0.00000000e+00  1.98128796e-05\n  1.99969848e+00  0.00000000e+00  1.93442938e-05  1.99969848e+00\n  0.00000000e+00  1.88244269e-05  1.99969848e+00  0.00000000e+00\n  1.82548306e-05  1.99969848e+00  0.00000000e+00  1.76371875e-05\n  1.99969848e+00  0.00000000e+00  1.69733060e-05  1.99969848e+00\n  0.00000000e+00  1.62651155e-05  1.99969848e+00  0.00000000e+00\n  1.55146602e-05  1.99969848e+00  0.00000000e+00  1.47240939e-05\n  1.99969848e+00  0.00000000e+00  1.38956733e-05  1.99969848e+00\n  0.00000000e+00  1.30317519e-05  1.99969848e+00  0.00000000e+00\n  1.21347732e-05  1.99969848e+00  0.00000000e+00  1.12072639e-05\n  1.99969848e+00  0.00000000e+00  1.02518268e-05  1.99969848e+00\n  0.00000000e+00  9.27113342e-06  1.99969848e+00  0.00000000e+00\n  8.26791681e-06  1.99969848e+00  0.00000000e+00  7.24496362e-06\n  1.99969848e+00  0.00000000e+00  6.20510661e-06  1.99969848e+00\n  0.00000000e+00  5.15121677e-06  1.99969848e+00  0.00000000e+00\n  4.08619540e-06  1.99969848e+00  0.00000000e+00  3.01296617e-06\n  1.99969848e+00  0.00000000e+00  1.93446711e-06  1.99969848e+00\n  0.00000000e+00  8.53642572e-07  1.99969848e+00  0.00000000e+00\n -2.26564761e-07  1.99969848e+00  0.00000000e+00 -1.30322188e-06\n  1.99969848e+00  0.00000000e+00 -2.37341341e-06  1.99969848e+00\n  0.00000000e+00 -3.43424942e-06  1.99969848e+00  0.00000000e+00\n -4.48287328e-06  1.99969848e+00  0.00000000e+00 -5.51646930e-06\n  1.99969848e+00  0.00000000e+00 -6.53227025e-06  1.99969848e+00\n  0.00000000e+00 -7.52756477e-06  1.99969848e+00  0.00000000e+00\n -8.49970457e-06  1.99969848e+00  0.00000000e+00 -9.44611148e-06\n  1.99969848e+00  0.00000000e+00 -1.03642842e-05  1.99969848e+00\n  0.00000000e+00 -1.12518050e-05  1.99969848e+00  0.00000000e+00\n -1.21063460e-05  1.99969848e+00  0.00000000e+00 -1.29256751e-05\n  1.99969848e+00  0.00000000e+00 -1.37076621e-05  1.99969848e+00\n  0.00000000e+00 -1.44502840e-05  1.99969848e+00  0.00000000e+00\n -1.51516301e-05  1.99969848e+00  0.00000000e+00 -1.58099072e-05\n  1.99969848e+00  0.00000000e+00 -1.64234439e-05  1.99969848e+00\n  0.00000000e+00 -1.69906949e-05  1.99969848e+00  0.00000000e+00\n -1.75102449e-05  1.99969848e+00  0.00000000e+00 -1.79808119e-05\n  1.99969848e+00  0.00000000e+00 -1.84012506e-05  1.99969848e+00\n  0.00000000e+00 -1.87705550e-05  1.99969848e+00  0.00000000e+00\n -1.90878607e-05  1.99969848e+00  0.00000000e+00 -1.93524468e-05\n  1.99969848e+00  0.00000000e+00 -1.95637378e-05  1.99969848e+00\n  0.00000000e+00 -1.97213045e-05  1.99969848e+00  0.00000000e+00]\n(180, 180)\n[[0.         2.         0.36227276]]\nfloat64\n"
    }
   ],
   "source": [
    "# Generate simulation\n",
    "dyn = d.PyDyn('test2.sim', time_length)\n",
    "state_init = dyn.compute(dyn.p_init)\n",
    "f = dyn.f(state_init, dyn.p_init)\n",
    "df = dyn.df_dp(state_init, dyn.p_init)\n",
    "dy = dyn.dy_dp(state_init, dyn.p_init)\n",
    "print(state_init.y)\n",
    "print(dy.shape)\n",
    "#Sample targets only variables in z direction\n",
    "y_target = np.zeros((samplenum, 3))\n",
    "y_target[:,2] = np.random.rand(samplenum)\n",
    "#x[:,0] = np.random.rand(samplenum)\n",
    "y_target[:,1] = 2\n",
    "print(y_target)\n",
    "p = dyn.get_p(y_target.transpose(), dyn.p_init)\n",
    "y_target= torch.tensor(y_target, requires_grad= True)\n",
    "print(p.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the custon Simulation Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulate(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        #print(f'input: {input.shape}')\n",
    "        p = input.clone().numpy().transpose()\n",
    "        state = dyn.compute(p)\n",
    "        y_pred = torch.tensor(state.y[-3:], requires_grad = True)\n",
    "        #print(f'y_pred: {y_pred.dtype}')\n",
    "        \n",
    "        ctx.save_for_backward(input)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        #print(grad_output.shape)\n",
    "        input, = ctx.saved_tensors\n",
    "        p = input.clone().numpy().transpose()\n",
    "        state= dyn.compute(p)\n",
    "        dy_dp = dyn.dy_dp(state, p)\n",
    "        dy_dp = dy_dp[-3:, :]\n",
    "        #print(f'shape of dy/dp: {dy_dp.shape}')\n",
    "        #print(f'shape of grad_output: {grad_output.shape}')\n",
    "        grad_output = grad_output.unsqueeze(0).t()\n",
    "        #print(f'shape of grad_output unsqueezed: {grad_output.shape}')\n",
    "        \n",
    "        grad_input = torch.tensor(dy_dp, requires_grad = True).t().mm(grad_output).t()\n",
    "        #grad_input = torch.tensor(dy_dp, requires_grad = True)\n",
    "        #print(f'shape of grad_input: {grad_input.dtype}')\n",
    "\n",
    "        return grad_input, None\n",
    "\n",
    "Simulate = Simulate.apply\n",
    "class ActiveLearn(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, out_sz):\n",
    "        super(ActiveLearn, self).__init__()\n",
    "\n",
    "        self.L_in = nn.Linear(n_in, 3*time_length).double()\n",
    "        self.Relu = nn.ReLU(inplace=True).double()\n",
    "        self.P = nn.Linear(3*time_length, 3*time_length).double()\n",
    "        #self.L_out = nn.Linear(3, 3)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.L_in(input)\n",
    "        x = self.Relu(x)\n",
    "        x = self.P(x)\n",
    "        x = self.Relu(x)\n",
    "        x, p = Simulate(x)\n",
    "        #x = self.L_out(x)\n",
    "        return x, p\n",
    "    \n",
    "model = ActiveLearn(input_size, output_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True\n"
    }
   ],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradcheck takes a tuple of tensors as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "#p = dyn.p_init\n",
    "p = torch.tensor(p, requires_grad = True)\n",
    "input = (p.double())\n",
    "#print(input)\n",
    "test = gradcheck(Simulate, (input,), eps=1e-6, atol=1e-7, raise_exception = True)\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}