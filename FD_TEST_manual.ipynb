{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import pydde as d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "samplenum = 1\n",
    "epochs = 200\n",
    "input_size = 3\n",
    "output_size = 3\n",
    "learning_rate = 0.01\n",
    "time_length = 3; #seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2.15365312e-05 1.99969848e+00 0.00000000e+00 2.15757288e-05\n 1.99969848e+00 0.00000000e+00 2.15562034e-05 1.99969848e+00\n 0.00000000e+00]\n(9, 9)\n[[0.         2.         0.82907117]]\nfloat64\n"
    }
   ],
   "source": [
    "# Generate simulation\n",
    "dyn = d.PyDyn('test2.sim', time_length)\n",
    "state_init = dyn.compute(dyn.p_init)\n",
    "f = dyn.py_f(state_init)\n",
    "df = dyn.py_df_dp(state_init)\n",
    "dy = dyn.py_dy_dp(state_init)\n",
    "print(state_init.y)\n",
    "print(dy.shape)\n",
    "#Sample targets only variables in z direction\n",
    "y_target = np.zeros((samplenum, 3))\n",
    "y_target[:,2] = np.random.rand(samplenum)\n",
    "#x[:,0] = np.random.rand(samplenum)\n",
    "y_target[:,1] = 2\n",
    "print(y_target)\n",
    "p = dyn.get_p(y_target.transpose(), dyn.p_init)\n",
    "y_target= torch.tensor(y_target, requires_grad= True)\n",
    "print(p.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the custon Simulation Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulate(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        #print(f'input: {input.shape}')\n",
    "        p = input.clone().numpy().transpose()\n",
    "        state = dyn.compute(p)\n",
    "        y_pred = torch.tensor(state.y[-3:], requires_grad = True)\n",
    "        #print(f'y_pred: {y_pred.dtype}')\n",
    "        \n",
    "        ctx.save_for_backward(input)\n",
    "        \n",
    "        return y_pred, input\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, input):\n",
    "        #print(grad_output.shape)\n",
    "        input, = ctx.saved_tensors\n",
    "        p = input.clone().numpy().transpose()\n",
    "        state= dyn.compute(p)\n",
    "        dy_dp = dyn.py_dy_dp(state)\n",
    "        dy_dp = dy_dp[-3:, :]\n",
    "        print(f'shape of dy/dp: {dy_dp.shape}')\n",
    "        print(f'shape of grad_output: {grad_output.shape}')\n",
    "        grad_output = grad_output.unsqueeze(0).t()\n",
    "        print(f'shape of grad_output unsqueezed: {grad_output.shape}')\n",
    "        \n",
    "        grad_input = torch.tensor(dy_dp, requires_grad = True).t().mm(grad_output).t()\n",
    "        #grad_input = torch.tensor(dy_dp, requires_grad = True)\n",
    "        print(f'shape of grad_input: {grad_input.dtype}')\n",
    "\n",
    "        return grad_input, None\n",
    "\n",
    "Simulate = Simulate.apply\n",
    "class ActiveLearn(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, out_sz):\n",
    "        super(ActiveLearn, self).__init__()\n",
    "\n",
    "        self.L_in = nn.Linear(n_in, 3*time_length).double()\n",
    "        self.Relu = nn.ReLU(inplace=True).double()\n",
    "        self.P = nn.Linear(3*time_length, 3*time_length).double()\n",
    "        #self.L_out = nn.Linear(3, 3)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.L_in(input)\n",
    "        x = self.Relu(x)\n",
    "        x = self.P(x)\n",
    "        x = self.Relu(x)\n",
    "        x, p = Simulate(x)\n",
    "        #x = self.L_out(x)\n",
    "        return x, p\n",
    "    \n",
    "model = ActiveLearn(input_size, output_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "shape of dy/dp: (3, 9)\nshape of grad_output: torch.Size([3])\nshape of grad_output unsqueezed: torch.Size([3, 1])\nshape of grad_input: torch.float64\ndy_dp shape = torch.Size([9])\ndy_dp = tensor([-0.0099, -0.1359, -0.0097,  0.0200,  0.1559,  0.0250,  0.0236,  0.9489,\n         0.0775], dtype=torch.float64)\n dy_dp_FD = [[-0.00987407 -0.1358899  -0.00967889  0.02002359  0.15594092  0.02497684\n   0.02363982  0.94892009  0.07746342]]\n dy_dp_FD = (1, 9)\n1.1611717294272575e-09\n"
    }
   ],
   "source": [
    "# Error for whole simulation\n",
    "from numpy import linalg as LA\n",
    "\n",
    "FE = 1e-6\n",
    "p = torch.tensor(p, requires_grad = True).double()\n",
    "y, p0 = Simulate(p)\n",
    "#test = sum(y)\n",
    "#test.backward()\n",
    "y.backward(torch.FloatTensor([1.0, 1.0, 1.0]).double())\n",
    "dy_dp = p.grad.double()\n",
    "print(f'dy_dp shape = {p.grad.shape}')\n",
    "print(f'dy_dp = {dy_dp}')\n",
    "dy_dp_FD = np.zeros((1,len(dyn.p_init)))\n",
    "#dy_dp_FD = np.zeros((3,len(dyn.p_init)))\n",
    "\n",
    "for i in range(len(dyn.p_init)):\n",
    "    dp= np.zeros(len(dyn.p_init))\n",
    "    dp[i] = FE\n",
    "    dp = torch.tensor(dp)\n",
    "    y_p, pp = Simulate(p + dp)\n",
    "    y_m, pm = Simulate(p - dp)\n",
    "    y_p = y_p.detach().numpy()\n",
    "    y_m = y_m.detach().numpy()\n",
    "    dy_dp_FD[0, i] = sum((y_p - y_m) / (2* FE))\n",
    "    #dy_dp_FD[:, i] = (y_p - y_m) / (2* FE)\n",
    "\n",
    "\n",
    "print(f' dy_dp_FD = {dy_dp_FD}')\n",
    "print(f' dy_dp_FD = {dy_dp_FD.shape}')\n",
    "dy_dp = dy_dp.detach().numpy()\n",
    "err = LA.norm(dy_dp_FD - dy_dp)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.0002908451455664775\n"
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "#Calculate dy_dp with FE\n",
    "FE = 1e-6\n",
    "dy_dp = sim.dy_dp(sim.y, sim.ydot, sim.yddot, sim.p)\n",
    "dy_dp_FD = np.zeros((len(sim.p),len(sim.p)))\n",
    "\n",
    "for i in range(len(sim.p)):\n",
    "    dp= np.zeros(len(sim.p))\n",
    "    dp[i] = FE\n",
    "    pos = sim.p + dp\n",
    "    y_p = sim.compute(pos)\n",
    "    y_m = sim.compute(sim.p - dp)\n",
    "    dy_dp_FD[:, i] = (y_p - y_m) / (2* FE)\n",
    "err = LA.norm(dy_dp_FD - dy_dp)\n",
    "print(err)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x\ndata: 1.0\nrequires_grad: True\ngrad: None\ngrad_fn: None\nis_leaf: True\n\ny\ndata: 2.0\nrequires_grad: False\ngrad: None\ngrad_fn: None\nis_leaf: True\n\nz\ndata: 2.0\nrequires_grad: True\ngrad: None\ngrad_fn: <MulBackward0 object at 0x1245503c8>\nis_leaf: False\n\n"
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "y = torch.tensor(2.0)\n",
    "z = x * y\n",
    "\n",
    "# Displaying\n",
    "for i, name in zip([x, y, z], \"xyz\"):\n",
    "    print(f\"{name}\\ndata: {i.data}\\nrequires_grad: {i.requires_grad}\\n\\\n",
    "grad: {i.grad}\\ngrad_fn: {i.grad_fn}\\nis_leaf: {i.is_leaf}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/os.py\n/Users/nicovonau/Code/thesis_pytorch/pydde.cpython-37m-darwin.so\n/Users/nicovonau/Library/Python/3.7/lib/python/site-packages/numpy/__init__.py\n/Users/nicovonau/Library/Python/3.7/lib/python/site-packages/torch/nn/__init__.py\n/Users/nicovonau/Library/Python/3.7/lib/python/site-packages/pandas/__init__.py\n"
    }
   ],
   "source": [
    "import os\n",
    "print(os.__file__)\n",
    "print(d.__file__)\n",
    "print(np.__file__)\n",
    "print(nn.__file__)\n",
    "print(pd.__file__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}